<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Spark on K8S</title>
    <url>/2021/03/23/Spark/Spark-On-K8S-1/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><a id="more"></a>





<h2 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h2><ul>
<li>Spark2.3版本以上</li>
<li>Kubernetes 版本&gt;= 1.6</li>
</ul>
<h2 id="How-it-works"><a href="#How-it-works" class="headerlink" title="How it works"></a>How it works</h2><p><img src="/images/spark/image013.png"></p>
<p>通过spark-submit可以直接想Kubernetes集群提交Spark application，提交过程包括以下：</p>
<ul>
<li>在Kubernetes pod中创建Spark driver.</li>
<li>driver在Kubernetes pod中创建executor，并连接它们，然后让它们执行application代码。</li>
<li>当Spark application执行完成，executor pods会被终止并清理，但是Driver的pod会持久化日志并且标记为“completed”状态，直到它被手动或者真正的垃圾清理回收掉。</li>
</ul>
<p>注：在commpleted状态的driver pod是不占用任何计算和内存资源的。</p>
<p>driver和executor运行的pod的调度是交由Kubernetes来管理的，客户端只需要通过api跟Kubernetes交互即可，可以通过配置属性来使用node selector把driver和executor的pod调度到一定范围内的可用节点上。当然了，以后的版本可能在pod的调度上，使用亲和性等是调度更加智能把。</p>
<h2 id="Submitting-Application-to-Kubernetes"><a href="#Submitting-Application-to-Kubernetes" class="headerlink" title="Submitting Application to Kubernetes"></a>Submitting Application to Kubernetes</h2><h3 id="Docker-Images"><a href="#Docker-Images" class="headerlink" title="Docker Images"></a>Docker Images</h3><p>Kubernetes需要用户提供pods内容器运行的镜像，镜像必须是Kubernetes支持的container运行时环境，Docker就是其中被使用广泛的一种container运行时。</p>
<p>Spark工程包下已经提供了Dockerfile</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;home&#x2F;penglei&#x2F;Bigdata&#x2F;spark&#x2F;resource-managers&#x2F;kubernetes&#x2F;docker&#x2F;src&#x2F;main&#x2F;dockerfiles&#x2F;spark</span><br></pre></td></tr></table></figure>
<p>Spark同样提供了docker-image-tool.sh脚本,可以直接用来构建发布Spark的docker image，默认情况下，通过脚本构建的JVM运行环境，可以通过option来更改。</p>
<p>简而言之，我们需要提供Spark的image，这个image可以使用Spark提供的脚本来创建。</p>
<h3 id="Cluster-Mode-Command"><a href="#Cluster-Mode-Command" class="headerlink" title="Cluster Mode Command"></a>Cluster Mode Command</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;spark-submit \</span><br><span class="line">    --master k8s:&#x2F;&#x2F;https:&#x2F;&#x2F;&lt;k8s-apiserver-host&gt;:&lt;k8s-apiserver-port&gt; \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --name spark-pi \</span><br><span class="line">    --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --conf spark.executor.instances&#x3D;5 \</span><br><span class="line">    --conf spark.kubernetes.container.image&#x3D;&lt;spark-image&gt; \</span><br><span class="line">    local:&#x2F;&#x2F;&#x2F;path&#x2F;to&#x2F;examples.jar</span><br></pre></td></tr></table></figure>
<p>–master参数必须是k8s开头，k8s的api server的url的校验可以看checkAndGetK8sMasterUrl函数</p>
<p>–name执行了Application的name</p>
<h3 id="Client-Mode"><a href="#Client-Mode" class="headerlink" title="Client Mode"></a>Client Mode</h3><p>Spark2.4.0版本支持client模式，</p>
<h3 id="Dependency-Management"><a href="#Dependency-Management" class="headerlink" title="Dependency Management"></a>Dependency Management</h3><ul>
<li>如果依赖项位于HDFS或者HTTP Server上，可以通过正确的URI来使用它们</li>
<li>可以把依赖项打入的dockerfile中，然后通过local://URI或者设置SPARK_EXTRA_CLASSPATH环境变量来使用，local://scheme是需要在spark-submit中指定的。</li>
<li>如果依赖项位于本地客户端，则可以使用file://scheme，然后指定上传的位置。官网有个例子</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">--packages org.apache.hadoop:hadoop-aws:3.2.2</span><br><span class="line">--conf spark.kubernetes.file.upload.path&#x3D;s3a:&#x2F;&#x2F;&lt;s3-bucket&gt;&#x2F;path</span><br><span class="line">--conf spark.hadoop.fs.s3a.access.key&#x3D;...</span><br><span class="line">--conf spark.hadoop.fs.s3a.impl&#x3D;org.apache.hadoop.fs.s3a.S3AFileSystem</span><br><span class="line">--conf spark.hadoop.fs.s3a.fast.upload&#x3D;true</span><br><span class="line">--conf spark.hadoop.fs.s3a.secret.key&#x3D;....</span><br><span class="line">--conf spark.driver.extraJavaOptions&#x3D;-Divy.cache.dir&#x3D;&#x2F;tmp -Divy.home&#x3D;&#x2F;tmp</span><br><span class="line">file:&#x2F;&#x2F;&#x2F;full&#x2F;path&#x2F;to&#x2F;app.jar</span><br></pre></td></tr></table></figure>
<p>此时app.jar会被上传到S3，当driver启动后，会下载到driver本地，并设置classpath。</p>
<h3 id="Pod-模板"><a href="#Pod-模板" class="headerlink" title="Pod 模板"></a>Pod 模板</h3><h3 id="K8S特性"><a href="#K8S特性" class="headerlink" title="K8S特性"></a>K8S特性</h3><ul>
<li><p>配置文件</p>
</li>
<li><p>Contexts</p>
</li>
<li><p>Namespace</p>
</li>
<li><p>RBAC</p>
</li>
</ul>
<h3 id="Spark-应用程序管理"><a href="#Spark-应用程序管理" class="headerlink" title="Spark 应用程序管理"></a>Spark 应用程序管理</h3><p>可以通过spark-submit CLI工具对运行在k8s中的application进行管理，可以参考<a href="https://issues.apache.org/jira/browse/SPARK-24793">SPARK-24793</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">eg1</span><br><span class="line">$ spark-submit --kill spark:spark-pi-1547948636094-driver --master k8s:&#x2F;&#x2F;https:&#x2F;&#x2F;192.168.2.8:8443</span><br><span class="line">eg2</span><br><span class="line">$ spark-submit --status spark:spark-pi-1547948636094-driver --master  k8s:&#x2F;&#x2F;https:&#x2F;&#x2F;192.168.2.8:8443</span><br><span class="line">eg3</span><br><span class="line">$ spark-submit --kill spark:spark-pi* --master  k8s:&#x2F;&#x2F;https:&#x2F;&#x2F;192.168.2.8:8443</span><br><span class="line"></span><br></pre></td></tr></table></figure>




<h3 id="Using-Kubernetes-Volumes"><a href="#Using-Kubernetes-Volumes" class="headerlink" title="Using Kubernetes Volumes"></a>Using Kubernetes Volumes</h3><pre><code> curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
 sudo install minikube-linux-amd64 /usr/local/bin/minikube
</code></pre>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
]]></content>
      <categories>
        <category>Spark On K8S系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark on K8S Submit分析</title>
    <url>/2021/03/23/Spark/Spark-On-K8S-2/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Spark已经集成了Kuburnetes资源管理框架，集成的方式是在Spark中启动一个客户端跟K8S的ApiServer进行通信，申请和释放资源来运行Driver和Executor。</p>
<a id="more"></a>

<p>关于submit这块儿的代码位于：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~&#x2F;Bigdata&#x2F;spark&#x2F;resource-managers&#x2F;kubernetes&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;deploy&#x2F;k8s&#x2F;submit</span><br></pre></td></tr></table></figure>


<h2 id="核心"><a href="#核心" class="headerlink" title="核心"></a>核心</h2><p>我们先看一张关于submit部分总的分析图</p>
<p><img src="/images/spark/image012.png"></p>
<p>从图中可以看出，在submit逻辑中，最核心的是KubernetesClientApplication的处理逻辑。</p>
<h3 id="KubernetesClientApplication"><a href="#KubernetesClientApplication" class="headerlink" title="KubernetesClientApplication"></a>KubernetesClientApplication</h3><p>一段来自代码中的注释</p>
<p><em>Main class and entry point of application submission in KUBERNETES mode.</em></p>
<p>关于如何从spark-submit脚本调用到这里，我们后续在“Spark on K8S 运行流程”分析。</p>
<ol>
<li><p>进入start方法，解析参数，调用run方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private[spark] class KubernetesClientApplication extends SparkApplication &#123;</span><br><span class="line"></span><br><span class="line">  override def start(args: Array[String], conf: SparkConf): Unit &#x3D; &#123;</span><br><span class="line">    val parsedArguments &#x3D; ClientArguments.fromCommandLineArgs(args)</span><br><span class="line">    run(parsedArguments, conf)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  private def run(clientArguments: ClientArguments, sparkConf: SparkConf): Unit &#x3D; &#123;</span><br><span class="line">    ...</span><br><span class="line">    val client &#x3D; new Client(</span><br><span class="line">         kubernetesConf,</span><br><span class="line">         new KubernetesDriverBuilder(),</span><br><span class="line">         kubernetesClient,</span><br><span class="line">         watcher)</span><br><span class="line">       client.run()</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
<li><p>创建Client对象，调用Client对象的run方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private[spark] class Client(</span><br><span class="line">    conf: KubernetesDriverConf,</span><br><span class="line">    builder: KubernetesDriverBuilder,</span><br><span class="line">    kubernetesClient: KubernetesClient,</span><br><span class="line">    watcher: LoggingPodStatusWatcher) extends Logging &#123;</span><br><span class="line"></span><br><span class="line">  def run(): Unit &#x3D; &#123;</span><br><span class="line">    ...</span><br><span class="line">    &#x2F;&#x2F;准备很多参数最终生成PodBuilder[resolvedDriverPod]，然后创建Driver</span><br><span class="line">    var watch: Watch &#x3D; null</span><br><span class="line">    val createdDriverPod &#x3D; kubernetesClient.pods().create(resolvedDriverPod)</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">    &#x2F;&#x2F;申请绑定Driver依赖的资源</span><br><span class="line">    try &#123;</span><br><span class="line">      val otherKubernetesResources &#x3D; resolvedDriverSpec.driverKubernetesResources ++ Seq(configMap)</span><br><span class="line">      addOwnerReference(createdDriverPod, otherKubernetesResources)</span><br><span class="line">      kubernetesClient.resourceList(otherKubernetesResources: _*).createOrReplace()</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case NonFatal(e) &#x3D;&gt;</span><br><span class="line">        kubernetesClient.pods().delete(createdDriverPod)</span><br><span class="line">        throw e</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    val sId &#x3D; Seq(conf.namespace, driverPodName).mkString(&quot;:&quot;)</span><br><span class="line">    &#x2F;&#x2F;通过Watcher监控Driver Pod的运行</span><br><span class="line">    breakable &#123;</span><br><span class="line">      while (true) &#123;</span><br><span class="line">        val podWithName &#x3D; kubernetesClient</span><br><span class="line">          .pods()</span><br><span class="line">          .withName(driverPodName)</span><br><span class="line">        watcher.reset()</span><br><span class="line">        watch &#x3D; podWithName.watch(watcher)</span><br><span class="line">        watcher.eventReceived(Action.MODIFIED, podWithName.get())</span><br><span class="line"></span><br><span class="line">        if(watcher.watchOrStop(sId)) &#123;</span><br><span class="line">          watch.close()</span><br><span class="line">          break</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


</li>
</ol>
<h3 id="LoggingPodStatusWatcher"><a href="#LoggingPodStatusWatcher" class="headerlink" title="LoggingPodStatusWatcher"></a>LoggingPodStatusWatcher</h3><p>LoggingPodStatusWatcher按照配置时间去监听Pod的状态</p>
<p>一段来自代码的注释</p>
<p> <em>A monitor for the running Kubernetes pod of a Spark application. Status logging occurs on every state change and also at an interval for liveness.</em></p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p><img src="/images/spark/image007.png"></p>
<h3 id="K8sSubmitOp"><a href="#K8sSubmitOp" class="headerlink" title="K8sSubmitOp"></a>K8sSubmitOp</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-24793">SPARK-24793</a>加强了spark-sbumit，更加有利于Application的管理.</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1674883">Spark submit分析</a></p>
]]></content>
      <categories>
        <category>Spark On K8S系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark on K8S Feature分析</title>
    <url>/2021/03/23/Spark/Spark-On-K8S-3/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在Spark App的执行过程中，有两种进程：Driver和Executor，这两种进程都会运行在K8S的Pod中，那么Driver和Executor运行的Pod环境是什么样的呢？又是如何定义呢？这就是feature包的作用了。</p>
<a id="more"></a>

<p>这篇博客所分析的代码路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~&#x2F;Bigdata&#x2F;spark&#x2F;resource-managers&#x2F;kubernetes&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;deploy&#x2F;k8s&#x2F;features</span><br></pre></td></tr></table></figure>
<h2 id="Driver和Executor的创建"><a href="#Driver和Executor的创建" class="headerlink" title="Driver和Executor的创建"></a>Driver和Executor的创建</h2><h3 id="Driver的创建"><a href="#Driver的创建" class="headerlink" title="Driver的创建"></a>Driver的创建</h3><ol>
<li><p>KubernetesClientApplication的入口方法start，调用自己的run 方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private[spark] class KubernetesClientApplication extends SparkApplication &#123;</span><br><span class="line"></span><br><span class="line">  override def start(args: Array[String], conf: SparkConf): Unit &#x3D; &#123;</span><br><span class="line">    val parsedArguments &#x3D; ClientArguments.fromCommandLineArgs(args)</span><br><span class="line">    run(parsedArguments, conf)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  private def run(clientArguments: ClientArguments, sparkConf: SparkConf): Unit &#x3D; &#123;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">   Utils.tryWithResource(SparkKubernetesClientFactory.createKubernetesClient(</span><br><span class="line">      master,</span><br><span class="line">      Some(kubernetesConf.namespace),</span><br><span class="line">      KUBERNETES_AUTH_SUBMISSION_CONF_PREFIX,</span><br><span class="line">      SparkKubernetesClientFactory.ClientType.Submission,</span><br><span class="line">      sparkConf,</span><br><span class="line">      None,</span><br><span class="line">      None)) &#123; kubernetesClient &#x3D;&gt;</span><br><span class="line">        &#x2F;&#x2F;创建client并运行</span><br><span class="line">        val client &#x3D; new Client(</span><br><span class="line">          kubernetesConf,</span><br><span class="line">          new KubernetesDriverBuilder(),</span><br><span class="line">          kubernetesClient,</span><br><span class="line">          watcher)</span><br><span class="line">        client.run()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
<li><p>Client的run方法创建Driver</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private[spark] class Client(</span><br><span class="line">    conf: KubernetesDriverConf,</span><br><span class="line">    builder: KubernetesDriverBuilder,</span><br><span class="line">    kubernetesClient: KubernetesClient,</span><br><span class="line">    watcher: LoggingPodStatusWatcher) extends Logging &#123;</span><br><span class="line"></span><br><span class="line">  def run(): Unit &#x3D; &#123;</span><br><span class="line">    &#x2F;&#x2F;重点-&gt; 构建Pod的Spec即buildFromFeatures</span><br><span class="line">    val resolvedDriverSpec &#x3D; builder.buildFromFeatures(conf, kubernetesClient)</span><br><span class="line">    ...</span><br><span class="line">    val resolvedDriverPod &#x3D; new PodBuilder(resolvedDriverSpec.pod.pod)</span><br><span class="line">      .editSpec()</span><br><span class="line">        .addToContainers(resolvedDriverContainer)</span><br><span class="line">        .addNewVolume()</span><br><span class="line">          .withName(SPARK_CONF_VOLUME_DRIVER)</span><br><span class="line">          .withNewConfigMap()</span><br><span class="line">            .withItems(KubernetesClientUtils.buildKeyToPathObjects(confFilesMap).asJava)</span><br><span class="line">            .withName(configMapName)</span><br><span class="line">            .endConfigMap()</span><br><span class="line">          .endVolume()</span><br><span class="line">        .endSpec()</span><br><span class="line">      .build()</span><br><span class="line">    ...</span><br><span class="line">    &#x2F;&#x2F;调用kubernetesClient去创建Pod</span><br><span class="line">    val createdDriverPod &#x3D; kubernetesClient.pods().create(resolvedDriverPod)</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


</li>
</ol>
<p>所以呢？<code>Driver</code>的创建依赖于<code>KubernetesDriverBuilder</code>的<code>buildFromFeatures</code>方法</p>
<h3 id="Executor的创建"><a href="#Executor的创建" class="headerlink" title="Executor的创建"></a>Executor的创建</h3><ol>
<li><p>ExecutorPodsAllocator的start方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def start(applicationId: String): Unit &#x3D; &#123;</span><br><span class="line">  snapshotsStore.addSubscriber(podAllocationDelay) &#123;</span><br><span class="line">    onNewSnapshots(applicationId, _)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
<li><p>ExecutorPodsAllocator的onNewSnapshots方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private def onNewSnapshots(</span><br><span class="line">    applicationId: String,</span><br><span class="line">    snapshots: Seq[ExecutorPodsSnapshot]): Unit &#x3D; &#123;</span><br><span class="line">    ...</span><br><span class="line">    if (newlyCreatedExecutorsForRpId.isEmpty</span><br><span class="line">      &amp;&amp; knownPodCount &lt; targetNum) &#123;</span><br><span class="line">      requestNewExecutors(targetNum, knownPodCount, applicationId, rpId)</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
<li><p>ExecutorPodsAllocator的requestNewExecutors方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private def requestNewExecutors(</span><br><span class="line">    expected: Int,</span><br><span class="line">    running: Int,</span><br><span class="line">    applicationId: String,</span><br><span class="line">    resourceProfileId: Int): Unit &#x3D; &#123;</span><br><span class="line">    ...</span><br><span class="line">    val resolvedExecutorSpec &#x3D; executorBuilder.buildFromFeatures(executorConf, secMgr,</span><br><span class="line">      kubernetesClient, rpIdToResourceProfile(resourceProfileId))      </span><br><span class="line">    ...</span><br><span class="line">    val podWithAttachedContainer &#x3D; new PodBuilder(executorPod.pod)</span><br><span class="line">      .editOrNewSpec()</span><br><span class="line">      .addToContainers(executorPod.container)</span><br><span class="line">      .endSpec()</span><br><span class="line">      .build()</span><br><span class="line">    val createdExecutorPod &#x3D; kubernetesClient.pods().create(podWithAttachedContainer)</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">   </span><br></pre></td></tr></table></figure>
<p>所以呢？<code>Executor</code>的创建依赖于<code>KubernetesExecutorBuilder</code>的<code>buildFromFeatures</code>方法</p>
</li>
</ol>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>不管是Driver还是Executor都是通过kubernetesClient客户端调用create的方法创建，只是传入的PodBuilder参数不同，创建时机不同。在构建PodBuilder的过程中，使用了feature包的各种配置进行组装。</p>
<p><img src="/images/spark/image010.png"></p>
<h2 id="feature"><a href="#feature" class="headerlink" title="feature"></a>feature</h2><p><img src="/images/spark/image011.png"></p>
<p>先看一下<code>KubernetesFeatureConfigStep</code>，这个是基础接口，后面的实现类其实非常简单，从类名上基本都识别出功能特性。</p>
<p>一段来自代码中的注释</p>
<p> <code>A collection of functions that together represent a &quot;feature&quot; in pods that are launched for Spark drivers and executors.</code></p>
<p>就是给Spark drivers 和 executors定义运行环境的功能集。如果我们想给它添加特定的运行环境，只需要定义一个功能子类实现<code>KubernetesFeatureConfigStep</code>接口即可。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1674888">Spark on K8S feature分析</a></p>
]]></content>
      <categories>
        <category>Spark On K8S系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark on K8S Scheduler分析</title>
    <url>/2021/03/23/Spark/Spark-On-K8S-4/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Spark Kubernetes采用了跟Yarn类似的方式集成到了Spark中，Kubernetes作为一个新的Resource manager供Spark使用，具体来说：</p>
<ol>
<li>通过继承<code>ExternalClusterManager</code>实现了<code>KubernetesClusterManager</code>的外部集群管理器。</li>
<li>通过继承<code>CoarseGrainedSchedulerBackend</code>实现`KubernetesClusterSchedulerBackend``资源调度器。</li>
</ol>
<a id="more"></a>

<p>本篇博客的源码目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~&#x2F;Bigdata&#x2F;spark&#x2F;resource-managers&#x2F;kubernetes&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;cluster&#x2F;k8s</span><br></pre></td></tr></table></figure>
<h2 id="核心"><a href="#核心" class="headerlink" title="核心"></a>核心</h2><p>先看一张概要图</p>
<p><img src="/images/spark/image008.png"></p>
<p>scheduler核心是KubernetesClusterManager和KubernetesClusterSchedulerBackend</p>
<h3 id="ClusterManager"><a href="#ClusterManager" class="headerlink" title="ClusterManager"></a>ClusterManager</h3><p>关于ExternalClusterManager：</p>
<p><em>A cluster manager interface to plugin external scheduler.</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;是否可以根据master URL创建集群管理器</span><br><span class="line">def canCreate(masterURL: String): Boolean</span><br><span class="line">&#x2F;&#x2F;创建TaskScheduler</span><br><span class="line">def createTaskScheduler(sc: SparkContext, masterURL: String): TaskScheduler</span><br><span class="line">&#x2F;&#x2F;创建SchedulerBackend,不同的集群管理器有不同的是实现，但一定继承自SchedulerBackend</span><br><span class="line">def createSchedulerBackend(sc: SparkContext,masterURL: String,scheduler: </span><br><span class="line">                            TaskScheduler): SchedulerBackend</span><br><span class="line">&#x2F;&#x2F;初始化TaskScheduler和SchedulerBackend                         </span><br><span class="line">def initialize(scheduler: TaskScheduler, backend: SchedulerBackend): Unit</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>所以想在Spark上对接一款自定义的集群管理器，其实还是比较简单的，只要实现这四个接口就行了。</p>
<h4 id="KubernetesClusterManager"><a href="#KubernetesClusterManager" class="headerlink" title="KubernetesClusterManager"></a>KubernetesClusterManager</h4><p><code>KubernetesClusterManager</code>作为Spark on K8S的管理中心，核心功能是负责<code>KubernetesClusterSchedulerBackend</code>的创建。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">override def createSchedulerBackend(</span><br><span class="line">    sc: SparkContext,</span><br><span class="line">    masterURL: String,</span><br><span class="line">    scheduler: TaskScheduler): SchedulerBackend &#x3D; &#123;</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  new KubernetesClusterSchedulerBackend(</span><br><span class="line">    scheduler.asInstanceOf[TaskSchedulerImpl],</span><br><span class="line">    sc,</span><br><span class="line">    kubernetesClient,</span><br><span class="line">    schedulerExecutorService,</span><br><span class="line">    snapshotsStore,</span><br><span class="line">    executorPodsAllocator,</span><br><span class="line">    executorPodsLifecycleEventHandler,</span><br><span class="line">    podsWatchEventSource,</span><br><span class="line">    podsPollingEventSource)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>TaskScheduler</p>
</li>
<li><p>SparkContext</p>
</li>
<li><p>KubernetesClient</p>
</li>
<li><p>ScheduledExecutorService</p>
</li>
<li><p>ExecutorPodsSnapshotsStore</p>
</li>
<li><p>ExecutorPodsAllocator</p>
</li>
<li><p>ExecutorPodsLifecycleManager</p>
</li>
<li><p>ExecutorPodsWatchSnapshotSource</p>
</li>
<li><p>ExecutorPodsPollingSnapshotSource</p>
</li>
</ul>
<h3 id="SchedulerBackend"><a href="#SchedulerBackend" class="headerlink" title="SchedulerBackend"></a>SchedulerBackend</h3><p><code>KubernetesClusterSchedulerBackend</code>继承自<code>CoarseGrainedSchedulerBackend</code>，其实Yarn和mesos的SchedulerBackend也是继承自<code>CoarseGrainedSchedulerBackend</code>，关于<code>CoarseGrainedSchedulerBackend</code></p>
<p>来自一段代码中的注释</p>
<p> <em>A scheduler backend that waits for coarse-grained executors to connect.This backend holds onto each executor for the duration of the Spark job rather than relinquishing executors whenever a task is done and asking the scheduler to launch a new executor for each new task. Executors may be launched in a variety of ways, such as Mesos tasks for the coarse-grained Mesos mode or standalone processes for Spark’s standalone deploy mode(spark.deploy.</em>).*</p>
<p>一个等待coarse-grained executors连接的调度程序后端，在Spark Job的执行期间这个调度后端保留所有的executor，而不是task执行介绍后释放executor，然后执行新的task的时候再申请executor。executor可以被多种资源管理器提供，比如mesos、standalone等。</p>
<p>通过不同的资源管理器，可以以更多的方式创建executor，当然包括Yarn、K8S等。</p>
<h4 id="DriverEndpoint"><a href="#DriverEndpoint" class="headerlink" title="DriverEndpoint"></a>DriverEndpoint</h4><p>这个顾名思义代表的Driver的endpoint，为什么要有这个呢？？？</p>
<p>其实可以想想Driver是要跟SchedulerBackend通信的</p>
<p>在K8S上的实现是<code>KubernetesDriverEndpoint</code></p>
<h4 id="KubernetesClusterSchedulerBackend"><a href="#KubernetesClusterSchedulerBackend" class="headerlink" title="KubernetesClusterSchedulerBackend"></a>KubernetesClusterSchedulerBackend</h4><p>主要负责executor的请求和kill以及删除。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>剩下的所有类定义包括接口的实现，其实都是在为KubernetesClusterSchedulerBackend服务</p>
<p><img src="/images/spark/image009.png"></p>
<p>如图，所有类都是围绕Executor Pod的管理，包括快照的、生命周期、创建删除等。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;Executor的构建类,仅仅是根据feature返回Executor的Spec</span><br><span class="line">KubernetesExecutorBuilder </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;Executor Pods Snapshot 的定义</span><br><span class="line">ExecutorPodsSnapshot</span><br><span class="line">&#x2F;&#x2F;Executor Pods状态 </span><br><span class="line">ExecutorPodState</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;Executor Pods Snapshot的存储接口</span><br><span class="line">ExecutorPodsSnapshotsStore</span><br><span class="line">&#x2F;&#x2F;Executor Pods Snapshot的存储接口实现类</span><br><span class="line">ExecutorPodsSnapshotsStoreImpl </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;监听Executor Pods Snapshot</span><br><span class="line">ExecutorPodsWatchSnapshotSource</span><br><span class="line">&#x2F;&#x2F;拉去Executor Pods Snapshot(定时)</span><br><span class="line">ExecutorPodsPollingSnapshotSource</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;Executor Pods生命周期管理</span><br><span class="line">ExecutorPodsLifecycleManager</span><br><span class="line">&#x2F;&#x2F;Executor Pods的分配</span><br><span class="line">ExecutorPodsAllocator </span><br></pre></td></tr></table></figure>


<p>总的来说，ExecutorPodsSnapshotsStoreImpl是一个生产者-消费者模型。在这个模型中：</p>
<p>生产者：</p>
<ul>
<li>ExecutorPodsWatchSnapshotSource通过继承K8S的Watcher，重写eventReceived方法，对Executor Pod进行监听，然后调用ExecutorPodsSnapshotsStore的updatePod方法，进行快照更新</li>
<li>ExecutorPodsPollingSnapshotSource通过30秒的定时任务，对Executor Pod进行监听，然后调用ExecutorPodsSnapshotsStore的replaceSnapshot方法，进行快照更新</li>
</ul>
<p>消费者：</p>
<ul>
<li><p>ExecutorPodsAllocator</p>
<p>  通过<code>ExecutorPodsSnapshotsStore</code>的<code>addSubscriber</code>进行订阅</p>
</li>
<li><p>ExecutorPodsLifecycleManager</p>
<p>  通过<code>ExecutorPodsSnapshotsStore</code>的<code>addSubscriber</code>进行订阅</p>
</li>
</ul>
<p><code>ExecutorPodsWatchSnapshotSource</code>,<code>ExecutorPodsPollingSnapshotSource</code>,<code>ExecutorPodsLifecycleManager</code>,<code>ExecutorPodsAllocator</code> 都包含了<code>KubernetesClient</code>，表示都要跟K8S的apiServer进行交互。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://cloud.tencent.com/developer/article/1674890">Spark K8S scheduler 分析</a></p>
<p><a href="https://github.com/apache/spark">Spark代码</a></p>
]]></content>
      <categories>
        <category>Spark On K8S系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark on K8S Shuffle分析</title>
    <url>/2021/03/23/Spark/Spark-On-K8S-5/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><a id="more"></a>



<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
]]></content>
      <categories>
        <category>Spark On K8S系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark on K8S 运行流程</title>
    <url>/2021/03/23/Spark/Spark-On-K8S-6/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>使用spark-submit如何提交一个application到Kubernetes集群使它运行起来，本篇博客将从源码的角度一步步分析整个流程。</p>
<a id="more"></a>

<p>从spark-submit到SparkApplication运行起来的时序如下图所示：</p>
<p><img src="/images/spark/image014.png" alt="image014"></p>
<p>我们依次对时序图中的每一个节点进行分析</p>
<h2 id="spark-submit"><a href="#spark-submit" class="headerlink" title="spark-submit"></a>spark-submit</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if [ -z &quot;$&#123;SPARK_HOME&#125;&quot; ]; then</span><br><span class="line">  # 设置环境变量</span><br><span class="line">  source &quot;$(dirname &quot;$0&quot;)&quot;&#x2F;find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># disable randomized hash for string in Python 3.3+</span><br><span class="line">export PYTHONHASHSEED&#x3D;0</span><br><span class="line">#调用spark-class脚本执行SparkSubmit类</span><br><span class="line">exec &quot;$&#123;SPARK_HOME&#125;&quot;&#x2F;bin&#x2F;spark-class org.apache.spark.deploy.SparkSubmit &quot;$@&quot;</span><br></pre></td></tr></table></figure>
<h2 id="spark-class"><a href="#spark-class" class="headerlink" title="spark-class"></a>spark-class</h2><ul>
<li>加载环境变量</li>
<li>找到执行RUNNER</li>
<li>找到Spark的Jar包</li>
<li>添加launcher到class path中</li>
<li>调用launcher.Main构造参数</li>
<li>exec “${CMD[@]}” 执行命令</li>
</ul>
<p>通过简单的spark-submit -h看看CMD最后的形式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;home&#x2F;penglei&#x2F;Tools&#x2F;java&#x2F;jdk&#x2F;bin&#x2F;java -cp &#x2F;home&#x2F;penglei&#x2F;Bigdata&#x2F;spark&#x2F;conf&#x2F;:&#x2F;home&#x2F;penglei&#x2F;Bigdata&#x2F;spark&#x2F;assembly&#x2F;target&#x2F;scala-2.12&#x2F;jars&#x2F;* -Xmx1g org.apache.spark.deploy.SparkSubmit --help</span><br></pre></td></tr></table></figure>
<p>可以看到就是java -cp 执行类而已</p>
<h2 id="launcher-Main"><a href="#launcher-Main" class="headerlink" title="launcher.Main"></a>launcher.Main</h2><p>一段来自代码中的注释</p>
<p><em>Command line interface for the Spark launcher. Used internally by Spark scripts.</em></p>
<p>在spark-class脚本中构建的CMD就是通过该类实现的。</p>
<h2 id="SparkSubmit-main"><a href="#SparkSubmit-main" class="headerlink" title="SparkSubmit.main"></a>SparkSubmit.main</h2><p>由spark-class脚本中最后的输出可以看到调用了org.apache.spark.deploy.SparkSubmit，SparkSubmit的main方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">override def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">  val submit &#x3D; new SparkSubmit() &#123;</span><br><span class="line">    self &#x3D;&gt;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  submit.doSubmit(args)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="SparkSubmit-doSubmit"><a href="#SparkSubmit-doSubmit" class="headerlink" title="SparkSubmit.doSubmit"></a>SparkSubmit.doSubmit</h2><p>可以看到目前SparkSubmit支持的Action一共由四种</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def doSubmit(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">  ...</span><br><span class="line">  appArgs.action match &#123;</span><br><span class="line">    case SparkSubmitAction.SUBMIT &#x3D;&gt; submit(appArgs, uninitLog)</span><br><span class="line">    case SparkSubmitAction.KILL &#x3D;&gt; kill(appArgs)</span><br><span class="line">    case SparkSubmitAction.REQUEST_STATUS &#x3D;&gt; requestStatus(appArgs)</span><br><span class="line">    case SparkSubmitAction.PRINT_VERSION &#x3D;&gt; printVersion()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="SparkSubmit-submit"><a href="#SparkSubmit-submit" class="headerlink" title="SparkSubmit.submit"></a>SparkSubmit.submit</h2><p>一段来自代码的注释</p>
<p><em>Submit the application using the provided parameters, ensuring to first wrap in a doAs when –proxy-user is specified.</em></p>
<p>最终调用的<code>doRunMain</code>方法</p>
<h2 id="SparkSubmit-doRunMain"><a href="#SparkSubmit-doRunMain" class="headerlink" title="SparkSubmit.doRunMain"></a>SparkSubmit.doRunMain</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def doRunMain(): Unit &#x3D; &#123;</span><br><span class="line">  if (args.proxyUser !&#x3D; null) &#123;</span><br><span class="line">    ...</span><br><span class="line">    runMain(args, uninitLog)</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    runMain(args, uninitLog)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>判断是否设置了执行代理用户，然后执行<code>runMain</code>方法</p>
<h2 id="SparkSubmit-runMain"><a href="#SparkSubmit-runMain" class="headerlink" title="SparkSubmit.runMain"></a>SparkSubmit.runMain</h2><p>一段来自代码中的注释</p>
<p><em>Run the main method of the child class using the submit arguments. This runs in two steps. First, we prepare the launch environment by setting up the appropriate classpath, system properties, and application arguments for running the child main class based on the cluster manager and the deploy mode. Second, we use this launch environment to invoke the main method of the child main class.</em> </p>
<p><em>Note that this main class will not be the one provided by the user if we’re running cluster deploy mode or python applications.</em></p>
<p>这里的child class是指用户提交的Application的class。这个方法涉及两个步骤</p>
<ul>
<li>设置launch环境信息，包括classpath、system properties、application arguments for cluster manager以及其他一些参数</li>
<li>反射加载主类，执行main方法</li>
</ul>
<p>用户的Application的class会被包装进JavaMainApplication来调用。</p>
<p>注意：如果是cluster deploy mode或者python的application的话，此时的main class不是用户提交的类，而是SparkApplication的子类。</p>
<p>具体关于这个child main class的含义，我们在下面的prepareSumitEnvironment分析。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private def runMain(args: SparkSubmitArguments, uninitLog: Boolean): Unit &#x3D; &#123;</span><br><span class="line">  &#x2F;&#x2F;step 1,准备launch环境信息</span><br><span class="line">  val (childArgs, childClasspath, sparkConf, childMainClass) &#x3D; prepareSubmitEnvironment(args)</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  var mainClass: Class[_] &#x3D; null</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line">  &#x2F;&#x2F;step2, 反射加载主类，执行main方法</span><br><span class="line">  val app: SparkApplication &#x3D; if (classOf[SparkApplication].isAssignableFrom(mainClass)) &#123;</span><br><span class="line">    mainClass.getConstructor().newInstance().asInstanceOf[SparkApplication]</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    new JavaMainApplication(mainClass)</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  try &#123;</span><br><span class="line">    app.start(childArgs.toArray, sparkConf)</span><br><span class="line">  &#125; catch &#123;</span><br><span class="line">    case t: Throwable &#x3D;&gt;</span><br><span class="line">      throw findCause(t)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="SparkSubmit-prepareSumitEnvironment"><a href="#SparkSubmit-prepareSumitEnvironment" class="headerlink" title="SparkSubmit.prepareSumitEnvironment"></a>SparkSubmit.prepareSumitEnvironment</h2><p>接着上面，我们分析prepareSubmitEnvironment方法</p>
<p>一段来自代码中的注释</p>
<p><em>Prepare the environment for submitting an application.</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@param args the parsed SparkSubmitArguments used for environment preparation.</span><br><span class="line">@param conf the Hadoop Configuration, this argument will only be set in unit test.</span><br><span class="line">@return a 4-tuple:</span><br><span class="line"> (1) the arguments for the child process,</span><br><span class="line"> (2) a list of classpath entries for the child,</span><br><span class="line"> (3) a map of system properties, and</span><br><span class="line"> (4) the main class for the child</span><br></pre></td></tr></table></figure>
<p>整个函数的逻辑都是围绕处理四个返回值来展开的，主要步骤包括：</p>
<ul>
<li>childArgs、childClasspath、sparkConf、childMainClass返回值四元组的定义</li>
<li>设置cluster manager</li>
<li>设置deploy mode</li>
<li>cluster manager的校验，比如类可加在，ApiServer可连接、Fail Through场景等。</li>
<li>update SparkConf,主要是用命令行参数覆盖SparkConf中的配置</li>
<li>根据不同cluster manager和deploy mode，下载远程依赖</li>
<li>赋值mainClass，不同类型的应用是不一样的。</li>
<li>Python类型应用和R类型应用的特殊处理</li>
<li>所有参数和命令行选项根据模式和cluster manager写入sparkConf中</li>
<li>不合法参数的处理</li>
<li>返回childArgs、childClasspath、sparkConf、childMainClass</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private[deploy] def prepareSubmitEnvironment(</span><br><span class="line">    args: SparkSubmitArguments,</span><br><span class="line">    conf: Option[HadoopConfiguration] &#x3D; None)</span><br><span class="line">    : (Seq[String], Seq[String], SparkConf, String) &#x3D; &#123;</span><br><span class="line">  &#x2F;&#x2F; 返回值定义</span><br><span class="line">  val childArgs &#x3D; new ArrayBuffer[String]()</span><br><span class="line">  val childClasspath &#x3D; new ArrayBuffer[String]()</span><br><span class="line">  val sparkConf &#x3D; args.toSparkConf()</span><br><span class="line">  var childMainClass &#x3D; &quot;&quot;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; 设置cluster manager</span><br><span class="line">  val clusterManager: Int &#x3D; args.master match &#123;</span><br><span class="line">    case &quot;yarn&quot; &#x3D;&gt; YARN</span><br><span class="line">    case m if m.startsWith(&quot;spark&quot;) &#x3D;&gt; STANDALONE</span><br><span class="line">    case m if m.startsWith(&quot;mesos&quot;) &#x3D;&gt; MESOS</span><br><span class="line">    case m if m.startsWith(&quot;k8s&quot;) &#x3D;&gt; KUBERNETES</span><br><span class="line">    case m if m.startsWith(&quot;local&quot;) &#x3D;&gt; LOCAL</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; 设置deploy mode; default is client mode</span><br><span class="line">  val deployMode: Int &#x3D; args.deployMode match &#123;</span><br><span class="line">    case &quot;client&quot; | null &#x3D;&gt; CLIENT</span><br><span class="line">    case &quot;cluster&quot; &#x3D;&gt; CLUSTER</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  if (clusterManager &#x3D;&#x3D; YARN) &#123;</span><br><span class="line">    &#x2F;&#x2F; Make sure YARN is included in our build if we&#39;re trying to use it</span><br><span class="line">    ...保证使用得到的类是可以加载的</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  if (clusterManager &#x3D;&#x3D; KUBERNETES) &#123;</span><br><span class="line">    args.master &#x3D; Utils.checkAndGetK8sMasterUrl(args.master)</span><br><span class="line">    &#x2F;&#x2F; Make sure KUBERNETES is included in our build if we&#39;re trying to use it</span><br><span class="line">    ...K8S集群可用</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Fail fast 场景</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  val isYarnCluster &#x3D; clusterManager &#x3D;&#x3D; YARN &amp;&amp; deployMode &#x3D;&#x3D; CLUSTER</span><br><span class="line">  val isMesosCluster &#x3D; clusterManager &#x3D;&#x3D; MESOS &amp;&amp; deployMode &#x3D;&#x3D; CLUSTER</span><br><span class="line">  val isStandAloneCluster &#x3D; clusterManager &#x3D;&#x3D; STANDALONE &amp;&amp; deployMode &#x3D;&#x3D; CLUSTER</span><br><span class="line">  val isKubernetesCluster &#x3D; clusterManager &#x3D;&#x3D; KUBERNETES &amp;&amp; deployMode &#x3D;&#x3D; CLUSTER</span><br><span class="line">  val isKubernetesClient &#x3D; clusterManager &#x3D;&#x3D; KUBERNETES &amp;&amp; deployMode &#x3D;&#x3D; CLIENT</span><br><span class="line">  val isKubernetesClusterModeDriver &#x3D; isKubernetesClient &amp;&amp;</span><br><span class="line">    sparkConf.getBoolean(&quot;spark.kubernetes.submitInDriver&quot;, false)</span><br><span class="line"></span><br><span class="line">  if (!isMesosCluster &amp;&amp; !isStandAloneCluster) &#123;</span><br><span class="line">    ...依赖项的下载</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; update spark config from args</span><br><span class="line">  args.toSparkConf(Option(sparkConf))</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Resolve glob path for different resources.</span><br><span class="line">  args.jars &#x3D; Option(args.jars).map(resolveGlobPaths(_, hadoopConf)).orNull</span><br><span class="line">  args.files &#x3D; Option(args.files).map(resolveGlobPaths(_, hadoopConf)).orNull</span><br><span class="line">  args.pyFiles &#x3D; Option(args.pyFiles).map(resolveGlobPaths(_, hadoopConf)).orNull</span><br><span class="line">  args.archives &#x3D; Option(args.archives).map(resolveGlobPaths(_, hadoopConf)).orNull</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; In client mode, download remote files.客户端模式需要下载远程文件</span><br><span class="line">  var localPrimaryResource: String &#x3D; null</span><br><span class="line">  var localJars: String &#x3D; null</span><br><span class="line">  var localPyFiles: String &#x3D; null</span><br><span class="line">  if (deployMode &#x3D;&#x3D; CLIENT) &#123;</span><br><span class="line">    localPrimaryResource &#x3D; Option(args.primaryResource).map &#123;</span><br><span class="line">      downloadFile(_, targetDir, sparkConf, hadoopConf)</span><br><span class="line">    &#125;.orNull</span><br><span class="line">    localJars &#x3D; Option(args.jars).map &#123;</span><br><span class="line">      downloadFileList(_, targetDir, sparkConf, hadoopConf)</span><br><span class="line">    &#125;.orNull</span><br><span class="line">    localPyFiles &#x3D; Option(args.pyFiles).map &#123;</span><br><span class="line">      downloadFileList(_, targetDir, sparkConf, hadoopConf)</span><br><span class="line">    &#125;.orNull</span><br><span class="line"></span><br><span class="line">    if (isKubernetesClusterModeDriver) &#123;</span><br><span class="line">      ...特殊处理</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">  if (clusterManager &#x3D;&#x3D; YARN) &#123;</span><br><span class="line">    ... Yarn模式的特殊处理</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F;开始尝试加载主类</span><br><span class="line">  &#x2F;&#x2F;第一种：java类型</span><br><span class="line">  if (args.mainClass &#x3D;&#x3D; null &amp;&amp; !args.isPython &amp;&amp; !args.isR) &#123;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F;第二种：python类型</span><br><span class="line">  if (args.isPython &amp;&amp; deployMode &#x3D;&#x3D; CLIENT) &#123;</span><br><span class="line">    if (args.primaryResource &#x3D;&#x3D; PYSPARK_SHELL) &#123;</span><br><span class="line">      args.mainClass &#x3D; &quot;org.apache.spark.api.python.PythonGatewayServer&quot;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      ...</span><br><span class="line">      args.mainClass &#x3D; &quot;org.apache.spark.deploy.PythonRunner&quot;</span><br><span class="line">      args.childArgs &#x3D; ArrayBuffer(localPrimaryResource, localPyFiles) ++ args.childArgs</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ...&#x2F;&#x2F;Python和R类型的特殊处理</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F;第三种：R类型</span><br><span class="line">  if (args.isR &amp;&amp; deployMode &#x3D;&#x3D; CLIENT) &#123;</span><br><span class="line">    if (args.primaryResource &#x3D;&#x3D; SPARKR_SHELL) &#123;</span><br><span class="line">      args.mainClass &#x3D; &quot;org.apache.spark.api.r.RBackend&quot;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      ...</span><br><span class="line">      args.mainClass &#x3D; &quot;org.apache.spark.deploy.RRunner&quot;</span><br><span class="line">      args.childArgs &#x3D; ArrayBuffer(localPrimaryResource) ++ args.childArgs</span><br><span class="line">      args.files &#x3D; mergeFileLists(args.files, args.primaryResource)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ...在客户端模式下，直接使用用户提交的主类，依赖的jar都放入cleasspath</span><br><span class="line">  &#x2F;&#x2F; In client mode, launch the application main class directly</span><br><span class="line">  &#x2F;&#x2F; In addition, add the main application jar and any added jars (if any) to the classpath</span><br><span class="line">  if (deployMode &#x3D;&#x3D; CLIENT) &#123;</span><br><span class="line">    childMainClass &#x3D; args.mainClass</span><br><span class="line">    if (localPrimaryResource !&#x3D; null &amp;&amp; isUserJar(localPrimaryResource)) &#123;</span><br><span class="line">      childClasspath +&#x3D; localPrimaryResource</span><br><span class="line">    &#125;</span><br><span class="line">    if (localJars !&#x3D; null) &#123; childClasspath ++&#x3D; localJars.split(&quot;,&quot;) &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">  &#x2F;&#x2F; Map all arguments to command-line options or system properties for our chosen mode</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; 各种特殊场景的处理</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; In yarn-cluster mode, use yarn.Client as a wrapper around the user class</span><br><span class="line">  &#x2F;&#x2F; 在Yarn Cluster模式下，需要使用yarn.Client作为wrapper包装用户的主类</span><br><span class="line">  if (isYarnCluster) &#123;</span><br><span class="line">    childMainClass &#x3D; YARN_CLUSTER_SUBMIT_CLASS</span><br><span class="line">    if (args.isPython) &#123;</span><br><span class="line">      childArgs +&#x3D; (&quot;--primary-py-file&quot;, args.primaryResource)</span><br><span class="line">      childArgs +&#x3D; (&quot;--class&quot;, &quot;org.apache.spark.deploy.PythonRunner&quot;)</span><br><span class="line">    &#125; else if (args.isR) &#123;</span><br><span class="line">      val mainFile &#x3D; new Path(args.primaryResource).getName</span><br><span class="line">      childArgs +&#x3D; (&quot;--primary-r-file&quot;, mainFile)</span><br><span class="line">      childArgs +&#x3D; (&quot;--class&quot;, &quot;org.apache.spark.deploy.RRunner&quot;)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      if (args.primaryResource !&#x3D; SparkLauncher.NO_RESOURCE) &#123;</span><br><span class="line">        childArgs +&#x3D; (&quot;--jar&quot;, args.primaryResource)</span><br><span class="line">      &#125;</span><br><span class="line">      childArgs +&#x3D; (&quot;--class&quot;, args.mainClass)</span><br><span class="line">    &#125;</span><br><span class="line">    if (args.childArgs !&#x3D; null) &#123;</span><br><span class="line">      args.childArgs.foreach &#123; arg &#x3D;&gt; childArgs +&#x3D; (&quot;--arg&quot;, arg) &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  if (isKubernetesCluster) &#123;</span><br><span class="line">    childMainClass &#x3D; KUBERNETES_CLUSTER_SUBMIT_CLASS</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Load any properties specified through --conf and the default properties file</span><br><span class="line">  for ((k, v) &lt;- args.sparkProperties) &#123;</span><br><span class="line">    sparkConf.setIfMissing(k, v)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line">  (childArgs.toSeq, childClasspath.toSeq, sparkConf, childMainClass)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>在client模式下，childMainClass = args.mainClass</p>
<p>在Standalone Cluster模式下，childMainClass  = RestSubmissionClientApp | ClientApp</p>
<p>在Yarn Cluster模式下，childMainClass  = YarnClusterApplication</p>
<p>在Mesos Cluster模式下，childMainClass  = RestSubmissionClientApp</p>
<p>在K8S Cluster模式下，childMainClass  = KubernetesClientApplication</p>
<p>childArgs是childMainClass的参数，childClasspath是childMainClass的classpath。</p>
<h2 id="SparkApplication-start"><a href="#SparkApplication-start" class="headerlink" title="SparkApplication.start"></a>SparkApplication.start</h2><p>一段来自代码中的注释</p>
<p><em>Entry point for a Spark application. Implementations must provide a no-argument</em> </p>
<p><em>constructor.</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private[spark] trait SparkApplication &#123;</span><br><span class="line">  def start(args: Array[String], conf: SparkConf): Unit</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="KubernetesClientApplication-start"><a href="#KubernetesClientApplication-start" class="headerlink" title="KubernetesClientApplication.start"></a>KubernetesClientApplication.start</h2><p>关于<code>KubernetesClientApplication</code>，一段来自代码中的注释</p>
<p><em>Main class and entry point of application submission in KUBERNETES mode.</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">override def start(args: Array[String], conf: SparkConf): Unit &#x3D; &#123;</span><br><span class="line">  val parsedArguments &#x3D; ClientArguments.fromCommandLineArgs(args)</span><br><span class="line">  run(parsedArguments, conf)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样，我们进入了K8S流程了，具体在K8S中怎么初始化，怎么申请资源，怎么监听资源等，可以继续顺着<code>run</code>方法继续看。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在Spark中Application是用户编写的Spark应用程序，Driver是驱动执行Spark Application的驱动器，这个驱动器如果位于客户机上，则是client模式，如果位于集群的某个节点上，则是cluster模式。ClusterManager，是一个集群管理器，北向负责跟Driver进行交互，南向负责跟真正的后端交互，比如Yarn、K8S、Mesos等。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
]]></content>
      <categories>
        <category>Spark On K8S系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark on K8S Demo尝试</title>
    <url>/2021/03/23/Spark/Spark-On-K8S-7/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><a id="more"></a>



<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
]]></content>
      <categories>
        <category>Spark On K8S系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 内置函数</title>
    <url>/2021/06/28/Spark/Spark-SQL-02/</url>
    <content><![CDATA[<p>在数据库中，内置函数是一个很重要的模块儿，它实现了一些通用但又重要的功能，比如：求<code>abs()</code>,    <code>sum()</code>, <code>avg()</code>等，在Spark SQL中也是一样的，Spark SQL提供了300+的内置函数来满足用户来使用，同时还有UDF来满足更多的用户场景、用户的需求。</p>
<a id="more"></a>

<p>在<code>FunctionRegistry.scala</code>中，<code>expressions</code> 使用MAP保存了当前所有的内置函数</p>
<p>注：最近想参与<code>Interval Type</code>的开发，在更多的内置函数中支持<code>Interval Type</code>类型可能是下一阶段的开发重点。尤其是<code>WIDTH_BUCKET</code>函数，个人理解它更应该支持<code>Interval Type</code>，比如统计持续时间的直方图</p>
<h2 id="内置函数"><a href="#内置函数" class="headerlink" title="内置函数"></a>内置函数</h2><h3 id="内置函数分类"><a href="#内置函数分类" class="headerlink" title="内置函数分类"></a>内置函数分类</h3><p>Spark SQL中内置函数大致分为：聚合函数、集合函数、日期函数、数学函数、混合函数、非聚合函数、排序函数、字符串函数、窗口函数、UDF函数等。</p>
<h3 id="内置函数使用"><a href="#内置函数使用" class="headerlink" title="内置函数使用"></a>内置函数使用</h3><p>假设有如下表和数据，使用lower内置函数</p>
<table>
<thead>
<tr>
<th>name</th>
<th>age</th>
<th>phone</th>
</tr>
</thead>
<tbody><tr>
<td>小明</td>
<td>18</td>
<td>1231231231</td>
</tr>
<tr>
<td>小红</td>
<td>19</td>
<td>1231231232</td>
</tr>
</tbody></table>
<ul>
<li><p>通过编程方式使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import org.apache.spark.sql.functions._</span><br><span class="line">df.select(lower(col(&quot;name&quot;)).as(&quot;name&quot;), col(&quot;age&quot;), col(&quot;phone&quot;)).show()</span><br></pre></td></tr></table></figure></li>
<li><p>通过SQL来使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark.sql(&quot;select lower(name) as name,age,phone from people&quot;).show()</span><br></pre></td></tr></table></figure>




</li>
</ul>
<h2 id="如何实现内置函数"><a href="#如何实现内置函数" class="headerlink" title="如何实现内置函数"></a>如何实现内置函数</h2><p>以<a href="https://github.com/apache/spark/pull/28764">SPARK-21117</a>为例：</p>
<ol>
<li><p>首先肯定是根据ANSI SQL去了解内置函数的行为</p>
</li>
<li><p>其次根据<code>function</code>的是实现的功能划分到对应的函数分类中，比如<code>width-buckets</code>就属于<code>math</code>类的函数</p>
</li>
<li><p>定义对应case class</p>
<ul>
<li><p>根据参数个数选择继承对应的表达式，比如<code>WidthBucket</code>根据SQL的描述一共会有4个输入参数，所以继承了<code>QuaternaryExpression</code></p>
</li>
<li><p>定义输入输出类型</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">override def inputTypes: Seq[AbstractDataType] &#x3D; Seq(DoubleType, DoubleType, DoubleType, LongType)</span><br><span class="line">override def dataType: DataType &#x3D; LongType</span><br></pre></td></tr></table></figure></li>
<li><p>重写求值方法</p>
</li>
<li><p>重写<code>doGenCode</code>方法</p>
</li>
<li><p>重写<code>withNewChildrenInternal</code>方法</p>
</li>
</ul>
</li>
<li><p>定义对应Object</p>
<p>在Object中定义求值方法</p>
</li>
<li><p>在<code>FunctionRegistry.scala</code>中注册该内置函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">expression[WidthBucket](&quot;width_bucket&quot;)</span><br></pre></td></tr></table></figure></li>
<li><p>补充测试用例</p>
</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><code>WIDTH_BUCKET</code>是在统计用的比较多的一个函数，它用于求一个<code>value</code>落在一个区间范围内的<code>buckets</code>中的某个<code>bucket</code>的编号。</p>
<p>目前Spark中的实现是支持double，返回long，其他数据类型可以通过Cast来转换使用，但是，我觉得像<code>DateType</code>、<code>Interval Type</code>更需要在<code>WIDTH_BUCKET</code>中支持，而且其他传统DB中的<code>WIDTH_BUCKET</code>支持的数据类型本身就比较多。</p>
<p>两个例子：</p>
<p>1、统计80后、90后、00后、10后的人数直方图，这个时候birthday就是DateType</p>
<p>2、统计任务持续时间为1天、2天、…的直方图，这个时候任务的持续时间是用Interval Type来表示的</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
]]></content>
      <categories>
        <category>Spark SQL 开发系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkSession</title>
    <url>/2021/03/23/Spark/Spark%E5%9F%BA%E7%A1%80-1/</url>
    <content><![CDATA[<p>从Spark2.0开始，Spark提供了新的统一的入口点SparkSession来使用Spark的各项功能，SparkSession封装了SparkConf、SparkContext、SQLContext、HiveContext等。</p>
<a id="more"></a>

<p>以SparkPi为例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">object SparkPi &#123;</span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    val spark &#x3D; SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;Spark Pi&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    val slices &#x3D; if (args.length &gt; 0) args(0).toInt else 2</span><br><span class="line">    val n &#x3D; math.min(100000L * slices, Int.MaxValue).toInt &#x2F;&#x2F; avoid overflow</span><br><span class="line">    val count &#x3D; spark.sparkContext.parallelize(1 until n, slices).map &#123; i &#x3D;&gt;</span><br><span class="line">      val x &#x3D; random * 2 - 1</span><br><span class="line">      val y &#x3D; random * 2 - 1</span><br><span class="line">      if (x*x + y*y &lt;&#x3D; 1) 1 else 0</span><br><span class="line">    &#125;.reduce(_ + _)</span><br><span class="line">    println(s&quot;Pi is roughly $&#123;4.0 * count &#x2F; (n - 1)&#125;&quot;)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看出SparkSession是使用Spark功能API的入口，采用的builder模式进行创建和使用</p>
<h2 id="Builder模式"><a href="#Builder模式" class="headerlink" title="Builder模式"></a>Builder模式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val spark &#x3D; SparkSession.builder</span><br></pre></td></tr></table></figure>
<p>其实就是返回了一个Builder对象</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def builder(): Builder &#x3D; new Builder</span><br></pre></td></tr></table></figure>
<p>在SparkSession的伴生对象中，有一个内部类Builder的定义</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Builder extends Logging &#123;</span><br><span class="line"></span><br><span class="line">    private[this] val options &#x3D; new scala.collection.mutable.HashMap[String, String]</span><br><span class="line"></span><br><span class="line">    private[this] val extensions &#x3D; new SparkSessionExtensions</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    def getOrCreate(): SparkSession &#x3D; synchronized</span><br><span class="line">    ...</span><br><span class="line">&#125;    </span><br></pre></td></tr></table></figure>
<p>可以看到Builder重要的功能点包括：</p>
<ul>
<li>设置options</li>
<li>设置extensions（扩展SparkSession属性）</li>
<li>获取或创建SparkSession</li>
</ul>
<h3 id="设置options"><a href="#设置options" class="headerlink" title="设置options"></a>设置options</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def config(key: String, value: String): Builder &#x3D; synchronized &#123;</span><br><span class="line">      options +&#x3D; key -&gt; value</span><br><span class="line">      this</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>各种重载的config方法</p>
<h3 id="设置extensions"><a href="#设置extensions" class="headerlink" title="设置extensions"></a>设置extensions</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * Inject extensions into the [[SparkSession]]. This allows a user to add Analyzer rules,</span><br><span class="line"> * Optimizer rules, Planning Strategies or a customized parser.</span><br><span class="line"> *</span><br><span class="line"> * @since 2.2.0</span><br><span class="line"> *&#x2F;</span><br><span class="line">def withExtensions(f: SparkSessionExtensions &#x3D;&gt; Unit): Builder &#x3D; synchronized &#123;</span><br><span class="line">  f(extensions)</span><br><span class="line">  this</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从注释上可以看到，扩展SparkSession，主要目的是允许用户添加分析器规则、优化器规则、执行计划策略和解析器。</p>
<h3 id="获取或创建SparkSession"><a href="#获取或创建SparkSession" class="headerlink" title="获取或创建SparkSession"></a>获取或创建SparkSession</h3><p>一段来自代码中的注释</p>
<p><em>Gets an existing [[SparkSession]] or, if there is no existing one, creates a new one based on the options set in this builder. This method first checks whether there is a valid thread-local SparkSession, and if yes, return that one. It then checks whether there is a valid global default SparkSession, and if yes, return that one. If no valid global default SparkSession exists, the method creates a new SparkSession and assigns the newly created SparkSession as the global default. In case an existing SparkSession is returned, the non-static config options specified in this builder will be applied to the existing SparkSession.</em></p>
<p>获取或创建一个SparkSession，如果存在可用的SparkSession的话，返回SparkSession并config动态配置，否则创建新的SparkSession</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def getOrCreate(): SparkSession &#x3D; synchronized &#123;</span><br><span class="line">      &#x2F;&#x2F;生成SparkConf</span><br><span class="line">      val sparkConf &#x3D; new SparkConf()</span><br><span class="line">      options.foreach &#123; case (k, v) &#x3D;&gt; sparkConf.set(k, v) &#125;</span><br><span class="line">      ...</span><br><span class="line">      &#x2F;&#x2F; Get the session from current thread&#39;s active session.</span><br><span class="line">      &#x2F;&#x2F; 返回当前Thread可用的SparkSession</span><br><span class="line">      var session &#x3D; activeThreadSession.get()</span><br><span class="line">      if ((session ne null) &amp;&amp; !session.sparkContext.isStopped) &#123;</span><br><span class="line">        applyModifiableSettings(session)</span><br><span class="line">        return session</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      &#x2F;&#x2F; Global synchronization so we will only set the default session once.</span><br><span class="line">      &#x2F;&#x2F; 获取全局可用的SparkSession或者创建SparkSession</span><br><span class="line">      SparkSession.synchronized &#123;</span><br><span class="line">        &#x2F;&#x2F; If the current thread does not have an active session, get it from the global session.</span><br><span class="line">        &#x2F;&#x2F; 获取全局可用的SparkSession</span><br><span class="line">        session &#x3D; defaultSession.get()</span><br><span class="line">        if ((session ne null) &amp;&amp; !session.sparkContext.isStopped) &#123;</span><br><span class="line">          applyModifiableSettings(session)</span><br><span class="line">          return session</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; No active nor global default session. Create a new one.</span><br><span class="line">        &#x2F;&#x2F; 创建SparkSession：分Step1、Step2、Step3</span><br><span class="line">        &#x2F;&#x2F; Step1: 创建SparkContext</span><br><span class="line">        val sparkContext &#x3D; userSuppliedContext.getOrElse &#123;</span><br><span class="line">          &#x2F;&#x2F; set a random app name if not given.</span><br><span class="line">          if (!sparkConf.contains(&quot;spark.app.name&quot;)) &#123;</span><br><span class="line">            sparkConf.setAppName(java.util.UUID.randomUUID().toString)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          SparkContext.getOrCreate(sparkConf)</span><br><span class="line">          &#x2F;&#x2F; Do not update &#96;SparkConf&#96; for existing &#96;SparkContext&#96;, as it&#39;s shared by all sessions.</span><br><span class="line">        &#125;</span><br><span class="line">        &#x2F;&#x2F; Step2: 应用扩展特性</span><br><span class="line">        applyExtensions(</span><br><span class="line">          sparkContext.getConf.get(StaticSQLConf.SPARK_SESSION_EXTENSIONS).getOrElse(Seq.empty),</span><br><span class="line">          extensions)</span><br><span class="line">        &#x2F;&#x2F; Step3: 创建SparkSession</span><br><span class="line">        session &#x3D; new SparkSession(sparkContext, None, None, extensions, options.toMap)</span><br><span class="line">        setDefaultSession(session)</span><br><span class="line">        setActiveSession(session)</span><br><span class="line">        registerContextListener(sparkContext)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      return session</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h2 id="SparkSession伴生类"><a href="#SparkSession伴生类" class="headerlink" title="SparkSession伴生类"></a>SparkSession伴生类</h2><p><em>The entry point to programming Spark with the Dataset and DataFrame API.</em></p>
<p>从上面的getOrCreate方法，我们可以了解到</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">new SparkSession(sparkContext, None, None, extensions, options.toMap)</span><br></pre></td></tr></table></figure>
<p>SparkSession入参有5个</p>
<ul>
<li>SparkContext： Spark上下文</li>
<li>Option[SharedState] ： 如果提供，则使用现有的共享状态</li>
<li>Option[SessionState]： 如果提供，继承会话状态，包括临时视图、SQL配置、UDF等。</li>
<li>SparkSessionExtensions： 扩展特性</li>
<li>Map[String, String] ： 配置conf集合</li>
</ul>
<h3 id="SharedState"><a href="#SharedState" class="headerlink" title="SharedState"></a>SharedState</h3><p><em>A class that holds all state shared across sessions in a given [[SQLContext]].</em></p>
<p>基于给定的SQLContext来维护所有跨session共享的状态。</p>
<h3 id="SessionState"><a href="#SessionState" class="headerlink" title="SessionState"></a>SessionState</h3><p><em>A class that holds all session-specific state in a given [[SparkSession]].</em></p>
<p>维护给定的SparkSession的session级别的状态。</p>
<ul>
<li>sharedState 跨会话共享的状态，如全局视图、外部的catalog</li>
<li>conf SQL的KV配置</li>
<li>experimentalMethods 实验方法接口，用于添加自定义规则策略和优化器</li>
<li>functionRegistry 函数注册目录，用于管理用户注册的函数</li>
<li>udfRegistration UDF注册接口，用于注册用户自定义函数</li>
<li>catalogBuilder 用于创建内部目录管理表和DB</li>
<li>sqlParser SQL解析器</li>
<li>analyzerBuilder SQL分析器构建器</li>
<li>optimizerBuilder SQL优化器</li>
<li>planner SQL执行计划</li>
<li>streamingQueryManagerBuilder 流查询管理器的Builder</li>
<li>resourceLoaderBuilder 资源加载器，用于加载跨session共享的jar或者文件</li>
<li>createQueryExecution 创建QueryExecution的函数对象</li>
<li>createClone 创建session副本的函数对象</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
]]></content>
      <categories>
        <category>Spark基础系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark SchedulerBackend</title>
    <url>/2021/03/26/Spark/Spark%E5%9F%BA%E7%A1%80-10/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在Spark中有一个核心模块调度器（Scheduler），这个核心模块Scheduler是在SparkContext的创建过程中完成初始化。这个Scheduler是面向应用的调度，跟ClusterManager面向资源的调度是由本质的区别的。</p>
<a id="more"></a>



<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
]]></content>
      <categories>
        <category>Spark基础系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark RDD</title>
    <url>/2021/03/23/Spark/Spark%E5%9F%BA%E7%A1%80-2/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><a id="more"></a>





<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
]]></content>
      <categories>
        <category>Spark基础系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkConf</title>
    <url>/2021/03/23/Spark/Spark%E5%9F%BA%E7%A1%80-3/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><a id="more"></a>





<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
]]></content>
      <categories>
        <category>Spark基础系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkContext</title>
    <url>/2021/03/23/Spark/Spark%E5%9F%BA%E7%A1%80-4/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><a id="more"></a>



<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
]]></content>
      <categories>
        <category>Spark基础系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkEnv机制</title>
    <url>/2021/03/23/Spark/Spark%E5%9F%BA%E7%A1%80-5/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>一段来自代码的注释：</p>
<p>“ <em>Holds all the runtime environment objects for a running Spark instance (either master or worker),including the serializer, RpcEnv, block manager, map output tracker, etc. Currently Spark code finds the SparkEnv through a global variable, so all the threads can access the same SparkEnv. It can be accessed by SparkEnv.get (e.g. after creating a SparkContext).</em> ”</p>
<a id="more"></a>

<p>SparkEnv是SparkContext对象中最重要的一个属性，保存着Spark运行实例的所有运行环境信息，包括序列化、RpcEnv，block 管理等。关于SparkContext这个大类我们后续分析。</p>
<h2 id="SparkEnv"><a href="#SparkEnv" class="headerlink" title="SparkEnv"></a>SparkEnv</h2><p>SparkEnv类的描述</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@DeveloperApi</span><br><span class="line">class SparkEnv (</span><br><span class="line">    val executorId: String,</span><br><span class="line">    private[spark] val rpcEnv: RpcEnv,</span><br><span class="line">    val serializer: Serializer,</span><br><span class="line">    val closureSerializer: Serializer,</span><br><span class="line">    val serializerManager: SerializerManager,</span><br><span class="line">    val mapOutputTracker: MapOutputTracker,</span><br><span class="line">    val shuffleManager: ShuffleManager,</span><br><span class="line">    val broadcastManager: BroadcastManager,</span><br><span class="line">    val blockManager: BlockManager,</span><br><span class="line">    val securityManager: SecurityManager,</span><br><span class="line">    val metricsSystem: MetricsSystem,</span><br><span class="line">    val memoryManager: MemoryManager,</span><br><span class="line">    val outputCommitCoordinator: OutputCommitCoordinator,</span><br><span class="line">    val conf: SparkConf) extends Logging &#123;</span><br><span class="line">    ...    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从SparkEnv的类的描述中可以看到SparkEnv一共包含了12个重要的组件</p>
<ul>
<li>RpcEnv 各组件间的通信环境信息</li>
<li>SerializerManager 对象的序列化管理器</li>
<li>MapOutputTracker 用于跟踪Map阶段任务的输出状态，此状态便于Reduce阶段任务获取地址及中间结果</li>
<li>ShuffleManager 负责Shuffle操作，包括Shuffle read和Shuffle write</li>
<li>BroadcastManager 广播变量管理器</li>
<li>BlockManager 块管理器，在Spark中Block可以理解为RDD的一个Partition</li>
<li>SecurityManager 主要对账户、权限及身份认证进行设置和管理</li>
<li>MetricsSystem 监控指标管理</li>
<li>MemoryManager 内存管理器，可以参考之前的Spark内存管理机制</li>
<li>OutputCommitCoordinator 确定任务是否可以把输出提到HDFS上</li>
</ul>
<p>SparkEnv从逻辑上可以分成DriverEnv还是ExecutorEnv</p>
<p>从堆栈中可以看到不管是DriverEnv还是ExecutorEnv最终的创建都会执行SparkEnv类中的create方法，正如代码中的注释所描述的：</p>
<p>“*Helper method to create a SparkEnv for a driver or an executor.*”</p>
<p><img src="/images/spark/image004.png"></p>
<p>整个create的过程就是SparkEnv所holds的运行环境信息对应manager的初始过程。</p>
<h2 id="SparkEnv的创建"><a href="#SparkEnv的创建" class="headerlink" title="SparkEnv的创建"></a>SparkEnv的创建</h2><p>前面章节也说了SparkEnv的创建就是每个组件的创建过程。挑重点学习一下。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val envInstance &#x3D; new SparkEnv(</span><br><span class="line">  executorId,</span><br><span class="line">  rpcEnv,</span><br><span class="line">  serializer,</span><br><span class="line">  closureSerializer,</span><br><span class="line">  serializerManager,</span><br><span class="line">  mapOutputTracker,</span><br><span class="line">  shuffleManager,</span><br><span class="line">  broadcastManager,</span><br><span class="line">  blockManager,</span><br><span class="line">  securityManager,</span><br><span class="line">  metricsSystem,</span><br><span class="line">  memoryManager,</span><br><span class="line">  outputCommitCoordinator,</span><br><span class="line">  conf)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Add a reference to tmp dir created by driver, we will delete this tmp dir when stop() </span><br><span class="line">   is</span><br><span class="line">&#x2F;&#x2F; called, and we only need to do it for driver. Because driver may run as a service, and </span><br><span class="line">   if we</span><br><span class="line">&#x2F;&#x2F; don&#39;t delete this tmp dir when sc is stopped, then will create too many tmp dirs.</span><br><span class="line">if (isDriver) &#123;</span><br><span class="line">  val sparkFilesDir &#x3D; Utils.createTempDir(Utils.getLocalDir(conf), </span><br><span class="line">                                          &quot;userFiles&quot;).getAbsolutePath</span><br><span class="line">  envInstance.driverTmpDir &#x3D; Some(sparkFilesDir)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">envInstance</span><br></pre></td></tr></table></figure>
<h3 id="广播管理器BroadcastManager"><a href="#广播管理器BroadcastManager" class="headerlink" title="广播管理器BroadcastManager"></a>广播管理器BroadcastManager</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val broadcastManager &#x3D; new BroadcastManager(isDriver, conf, securityManager)</span><br></pre></td></tr></table></figure>
<h3 id="Map任务输出跟踪器MapOutputTracker"><a href="#Map任务输出跟踪器MapOutputTracker" class="headerlink" title="Map任务输出跟踪器MapOutputTracker"></a>Map任务输出跟踪器MapOutputTracker</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val mapOutputTracker &#x3D; if (isDriver) &#123;</span><br><span class="line">  new MapOutputTrackerMaster(conf, broadcastManager, isLocal)</span><br><span class="line">&#125; else &#123;</span><br><span class="line">  new MapOutputTrackerWorker(conf)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Have to assign trackerEndpoint after initialization as MapOutputTrackerEndpoint</span><br><span class="line">&#x2F;&#x2F; requires the MapOutputTracker itself</span><br><span class="line">mapOutputTracker.trackerEndpoint &#x3D; registerOrLookupEndpoint(MapOutputTracker.ENDPOINT_NAME,</span><br><span class="line">  new MapOutputTrackerMasterEndpoint(</span><br><span class="line">	rpcEnv, mapOutputTracker.asInstanceOf[MapOutputTrackerMaster], conf))</span><br></pre></td></tr></table></figure>
<h3 id="Shuffle管理器ShuffleManager"><a href="#Shuffle管理器ShuffleManager" class="headerlink" title="Shuffle管理器ShuffleManager"></a>Shuffle管理器ShuffleManager</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; Let the user specify short names for shuffle managers</span><br><span class="line">val shortShuffleMgrNames &#x3D; Map(</span><br><span class="line">  &quot;sort&quot; -&gt; classOf[org.apache.spark.shuffle.sort.SortShuffleManager].getName,</span><br><span class="line">  &quot;tungsten-sort&quot; -&gt; classOf[org.apache.spark.shuffle.sort.SortShuffleManager].getName)</span><br><span class="line">val shuffleMgrName &#x3D; conf.get(config.SHUFFLE_MANAGER)</span><br><span class="line">val shuffleMgrClass &#x3D;</span><br><span class="line">  shortShuffleMgrNames.getOrElse(shuffleMgrName.toLowerCase(Locale.ROOT), shuffleMgrName)</span><br><span class="line">val shuffleManager &#x3D; instantiateClass[ShuffleManager](shuffleMgrClass)</span><br></pre></td></tr></table></figure>
<h3 id="内存管理器MemoryManager"><a href="#内存管理器MemoryManager" class="headerlink" title="内存管理器MemoryManager"></a>内存管理器MemoryManager</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val memoryManager: MemoryManager &#x3D; UnifiedMemoryManager(conf, numUsableCores)</span><br></pre></td></tr></table></figure>
<h3 id="BlockManagerMaster"><a href="#BlockManagerMaster" class="headerlink" title="BlockManagerMaster"></a>BlockManagerMaster</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; Mapping from block manager id to the block manager&#39;s information.</span><br><span class="line">val blockManagerInfo &#x3D; new concurrent.TrieMap[BlockManagerId, BlockManagerInfo]()</span><br><span class="line">val blockManagerMaster &#x3D; new BlockManagerMaster(</span><br><span class="line">  registerOrLookupEndpoint(</span><br><span class="line">	BlockManagerMaster.DRIVER_ENDPOINT_NAME,</span><br><span class="line">	new BlockManagerMasterEndpoint(</span><br><span class="line">	  rpcEnv,</span><br><span class="line">	  isLocal,</span><br><span class="line">	  conf,</span><br><span class="line">	  listenerBus,</span><br><span class="line">	  if (conf.get(config.SHUFFLE_SERVICE_FETCH_RDD_ENABLED)) &#123;</span><br><span class="line">		externalShuffleClient</span><br><span class="line">	  &#125; else &#123;</span><br><span class="line">		None</span><br><span class="line">	  &#125;, blockManagerInfo,</span><br><span class="line">	  mapOutputTracker.asInstanceOf[MapOutputTrackerMaster])),</span><br><span class="line">  registerOrLookupEndpoint(</span><br><span class="line">	BlockManagerMaster.DRIVER_HEARTBEAT_ENDPOINT_NAME,</span><br><span class="line">	new BlockManagerMasterHeartbeatEndpoint(rpcEnv, isLocal, blockManagerInfo)),</span><br><span class="line">  conf,</span><br><span class="line">  isDriver)</span><br></pre></td></tr></table></figure>
<h3 id="BlockManager"><a href="#BlockManager" class="headerlink" title="BlockManager"></a>BlockManager</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; NB: blockManager is not valid until initialize() is called later.</span><br><span class="line">val blockManager &#x3D; new BlockManager(</span><br><span class="line">  executorId,</span><br><span class="line">  rpcEnv,</span><br><span class="line">  blockManagerMaster,</span><br><span class="line">  serializerManager,</span><br><span class="line">  conf,</span><br><span class="line">  memoryManager,</span><br><span class="line">  mapOutputTracker,</span><br><span class="line">  shuffleManager,</span><br><span class="line">  blockTransferService,</span><br><span class="line">  securityManager,</span><br><span class="line">  externalShuffleClient)</span><br></pre></td></tr></table></figure>


<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
<p><a href="https://cnblogs.com/xia520pi/p/8609625.html">SparkEnv详解</a></p>
]]></content>
      <categories>
        <category>Spark基础系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark之Shuffle</title>
    <url>/2021/03/23/Spark/Spark%E5%9F%BA%E7%A1%80-6/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Shuffle过程是进行数据洗牌的过程，由于涉及到了磁盘IO、数据序列化于反序列化、网络IO，所以Shuffle过程是一个非常消耗资源的过程，所以对Shuffle有优化会对大数据计算框架带来显著的性能提升。</p>
<a id="more"></a>

<p>Shuffle分为write和read两种操作，Map阶段（shuffle write）会将Map Task的结果数据写入到特定地方，比如内存、Disk、第三方存储介质、或者直接push到reduce 端。Reduce阶段（shuffle read）从固定的地方读取数据（前面shuffle write的数据）。</p>
<h2 id="Shuffle历史"><a href="#Shuffle历史" class="headerlink" title="Shuffle历史"></a>Shuffle历史</h2><p>Spark V1.1 之前使用的是Hash base shuffle</p>
<p>Spark V1.1 之后使用的是Sort base shuffle</p>
<p>Spark V1.6 之后Tungsten Sort并入Sort base shuffle</p>
<p>Spark V2.0 废弃Hash base shuffle</p>
<p>关于Spark Shuffle 网上有非常多的资料介绍Hash base shuffle以及各种Shuffle机制。由于目前Spark（V3.0）只有Sort base shuffle，所以本文只介绍Sort base shuffle，摒弃过时的分析，减少各类对比，专注目前Spark shuffle的实现。</p>
<h2 id="ShuffleManager"><a href="#ShuffleManager" class="headerlink" title="ShuffleManager"></a>ShuffleManager</h2><p>一段来自代码的注释：</p>
<p><em>“ Pluggable interface for shuffle systems. A ShuffleManager is created in SparkEnv on the driver and on each executor, based on the spark.shuffle.manager setting. The driver registers shuffles with it, and executors (or tasks running locally in the driver) can ask to read and write data. ”</em></p>
<p>这个接口比较简单</p>
<p><img src="/images/spark/image005.png"></p>
<p>NOTE:</p>
<ol>
<li>This will be instantiated by SparkEnv so its constructor can take a SparkConf and boolean isDriver as parameters.</li>
<li>This contains a method ShuffleBlockResolver which interacts with External Shuffle Service when it is enabled. Need to pay attention to that, if implementing a custom ShuffleManager, to make sure the custom ShuffleManager could co-exist with External Shuffle Service.</li>
</ol>
<h2 id="SortShuffleManager"><a href="#SortShuffleManager" class="headerlink" title="SortShuffleManager"></a>SortShuffleManager</h2><p>ShuffleManager的唯一实现SortShuffleManager</p>
<p><img src="/images/spark/image006.png"></p>
<p>最核心的三个操作：</p>
<ol>
<li>getWriter（获取Shuffle write的执行器，不同的执行器性能、使用场景、使用条件都不一样）</li>
<li>getReader （获取Shuffle read的执行器）</li>
<li>ShuffleBlockResolver-&gt;IndexShuffleBlockResolver （跟External Shuffle Service交互的组件，当然了这个必须是使用了External Shuffle Service的条件下）</li>
</ol>
<h3 id="Shuffle-Write"><a href="#Shuffle-Write" class="headerlink" title="Shuffle Write"></a>Shuffle Write</h3><p><em>“Obtained inside a map task to write out records to the shuffle system.”</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private[spark] abstract class ShuffleWriter[K, V] &#123;</span><br><span class="line">  &#x2F;** Write a sequence of records to this task&#39;s output *&#x2F;</span><br><span class="line">  @throws[IOException]</span><br><span class="line">  def write(records: Iterator[Product2[K, V]]): Unit</span><br><span class="line"></span><br><span class="line">  &#x2F;** Close this writer, passing along whether the map completed *&#x2F;</span><br><span class="line">  def stop(success: Boolean): Option[MapStatus]</span><br><span class="line"></span><br><span class="line">  &#x2F;** Get the lengths of each partition *&#x2F;</span><br><span class="line">  def getPartitionLengths(): Array[Long]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>目前Shuffle write一种有三种</p>
<ul>
<li>SortShuffleWriter</li>
<li>BypassMergeSortShuffleWriter</li>
<li>UnsafeShuffleWriter</li>
</ul>
<p>我们依次分析三种writer的write方法，总结不同之处。</p>
<h4 id="SortShuffleWriter"><a href="#SortShuffleWriter" class="headerlink" title="SortShuffleWriter"></a>SortShuffleWriter</h4><p>重点分析write方法，代码有精简</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sorter &#x3D; new ExternalSorter[K, V, C]()</span><br><span class="line"></span><br><span class="line">sorter.insertAll(records)</span><br><span class="line"></span><br><span class="line">val mapOutputWriter &#x3D; shuffleExecutorComponents.createMapOutputWriter(</span><br><span class="line">      dep.shuffleId, mapId, dep.partitioner.numPartitions)</span><br><span class="line">    </span><br><span class="line">sorter.writePartitionedMapOutput(dep.shuffleId, mapId, mapOutputWriter)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol>
<li>创建ExternalSorter类型的sorter对象</li>
<li>调入insertAll方法写入records</li>
<li>创建mapOutputWriter</li>
<li>写入结果数据</li>
</ol>
<p>所以对于SortShuffleWriter来说，最重要的ExternalSorter类型的sorter对象。关于它，后续专门分析。</p>
<h4 id="BypassMergeSortShuffleWriter"><a href="#BypassMergeSortShuffleWriter" class="headerlink" title="BypassMergeSortShuffleWriter"></a>BypassMergeSortShuffleWriter</h4><p>重点分析write方法，代码有精简</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void write(Iterator&lt;Product2&lt;K, V&gt;&gt; records) throws IOException &#123;</span><br><span class="line">  ...</span><br><span class="line">  ShuffleMapOutputWriter mapOutputWriter &#x3D; shuffleExecutorComponents</span><br><span class="line">      .createMapOutputWriter(shuffleId, mapId, numPartitions);</span><br><span class="line">  try &#123;</span><br><span class="line">    ...</span><br><span class="line">    final SerializerInstance serInstance &#x3D; serializer.newInstance();</span><br><span class="line">    final long openStartTime &#x3D; System.nanoTime();</span><br><span class="line">    </span><br><span class="line">    partitionWriters &#x3D; new DiskBlockObjectWriter[numPartitions];</span><br><span class="line">    partitionWriterSegments &#x3D; new FileSegment[numPartitions];</span><br><span class="line">    </span><br><span class="line">    for (int i &#x3D; 0; i &lt; numPartitions; i++) &#123;</span><br><span class="line">      ...</span><br><span class="line">      &#x2F;&#x2F;对每个Partitition创建临时文件和DiskBlockObjectWriter</span><br><span class="line">      final File file &#x3D; tempShuffleBlockIdPlusFile._2();</span><br><span class="line">      partitionWriters[i] &#x3D;</span><br><span class="line">          blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">    &#x2F;&#x2F;写数据</span><br><span class="line">    while (records.hasNext()) &#123;</span><br><span class="line">      final Product2&lt;K, V&gt; record &#x3D; records.next();</span><br><span class="line">      final K key &#x3D; record._1();</span><br><span class="line">      partitionWriters[partitioner.getPartition(key)].write(key, record._2());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    for (int i &#x3D; 0; i &lt; numPartitions; i++) &#123;</span><br><span class="line">      try (DiskBlockObjectWriter writer &#x3D; partitionWriters[i]) &#123;</span><br><span class="line">        partitionWriterSegments[i] &#x3D; writer.commitAndGet();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;文件进行了合并</span><br><span class="line">    partitionLengths &#x3D; writePartitionedData(mapOutputWriter);</span><br><span class="line">    </span><br><span class="line">    mapStatus &#x3D; MapStatus$.MODULE$.apply(</span><br><span class="line">      blockManager.shuffleServerId(), partitionLengths, mapId);</span><br><span class="line">  &#125; catch (Exception e) &#123;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>创建ShuffleMapOutputWriter类型的mapOutputWriter</li>
<li>针对每个Partition创建临时文件和DiskBlockObjectWriter</li>
<li>写入所有record</li>
<li>把所有的临时文件用mapOutputWriter合并，并删除临时文件</li>
</ol>
<h4 id="UnsafeShuffleWriter"><a href="#UnsafeShuffleWriter" class="headerlink" title="UnsafeShuffleWriter"></a>UnsafeShuffleWriter</h4><p>重点分析write方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void write(scala.collection.Iterator&lt;Product2&lt;K, V&gt;&gt; records) throws IOException   &#123;</span><br><span class="line">  ...</span><br><span class="line">  boolean success &#x3D; false;</span><br><span class="line">  try &#123;</span><br><span class="line">    while (records.hasNext()) &#123;</span><br><span class="line">      insertRecordIntoSorter(records.next());</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F;数据最后merge一把，同时生成索引文件</span><br><span class="line">    closeAndWriteOutput();</span><br><span class="line">    success &#x3D; true;</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    sorter.cleanupResources();</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>insertRecordIntoSorter,所有record写入sorter</li>
<li>closeAndWriteOutput</li>
<li>修改标记</li>
</ol>
<p>insertRecordIntoSorter：将数据写入到排序器中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">void insertRecordIntoSorter(Product2&lt;K, V&gt; record) throws IOException &#123;</span><br><span class="line">  ...</span><br><span class="line">  serBuffer.reset();</span><br><span class="line">  serOutputStream.writeKey(key, OBJECT_CLASS_TAG);</span><br><span class="line">  serOutputStream.writeValue(record._2(), OBJECT_CLASS_TAG);</span><br><span class="line">  serOutputStream.flush();</span><br><span class="line"></span><br><span class="line">  final int serializedRecordSize &#x3D; serBuffer.size();</span><br><span class="line">  ...</span><br><span class="line">  sorter.insertRecord(</span><br><span class="line">    serBuffer.getBuf(), Platform.BYTE_ARRAY_OFFSET, serializedRecordSize, partitionId);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>closeAndWriteOutput: 将所有的临时文件合并成一个文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">void closeAndWriteOutput() throws IOException &#123;</span><br><span class="line">  ...</span><br><span class="line">  try &#123;</span><br><span class="line">    partitionLengths &#x3D; mergeSpills(spills);</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">  mapStatus &#x3D; MapStatus$.MODULE$.apply(</span><br><span class="line">    blockManager.shuffleServerId(), partitionLengths, mapId);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>SortShuffleWriter包含ExternalSorter、ShuffleMapOutputWriter</p>
<p>BypassMergeSortShuffleWriter包含ShuffleMapOutputWriter，没有sorter排序器</p>
<p>UnsafeShuffleWriter包含ShuffleExternalSorter，没有ShuffleMapOutputWriter但是通过ShuffleBlockResolver生成了索引文件。</p>
<p>三种writer的写入机制不同，使用场景和条件不同，但是最终都会有临时文件的合并操作，合并成一个文件，同时又mapStatus状态的统计、partitionLengths的统计等。</p>
<h3 id="Shuffle-Read"><a href="#Shuffle-Read" class="headerlink" title="Shuffle Read"></a>Shuffle Read</h3><p>来自代码的注释</p>
<p><em>“Obtained inside a reduce task to read combined records from the mappers.”</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private[spark] trait ShuffleReader[K, C] &#123;</span><br><span class="line">  &#x2F;** Read the combined key-values for this reduce task *&#x2F;</span><br><span class="line">  def read(): Iterator[Product2[K, C]]</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; def stop(): Unit</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在目前的Shuffle的实现中，只有一种Shuffle Reader，即：BlockStoreShuffleReader</p>
<h4 id="BlockStoreShuffleReader"><a href="#BlockStoreShuffleReader" class="headerlink" title="BlockStoreShuffleReader"></a>BlockStoreShuffleReader</h4><p>来自代码的注释</p>
<p><em>“Fetches and reads the blocks from a shuffle by requesting them from other nodes’ block stores.”</em></p>
<p>TODO</p>
<h3 id="ShuffleBlockResolver"><a href="#ShuffleBlockResolver" class="headerlink" title="ShuffleBlockResolver"></a>ShuffleBlockResolver</h3><p>TODO</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
]]></content>
      <categories>
        <category>Spark基础系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark MemoryManager</title>
    <url>/2021/03/23/Spark/Spark%E5%9F%BA%E7%A1%80-7/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Spark 作为一个基于内存的分布式计算引擎，其内存管理模块在整个系统中扮演着非常重要的角色。理解 Spark 内存管理的基本原理，有助于更好地开发 Spark 应用程序和进行性能调优。</p>
<a id="more"></a>

<p>在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能。</p>
<p>Spark内存管理代码主要代码分布在org.apache.spark.memory包下。</p>
<h2 id="内存分类"><a href="#内存分类" class="headerlink" title="内存分类"></a>内存分类</h2><p>在Spark中关于内存管理，入口类：<strong>MemoryManager</strong></p>
<p>从MemoryManager中可以总结出Spark的内存一共有两种划分方式：</p>
<ol>
<li><p>从内存的作用来划分（MemoryPool）；</p>
<p>MemoryPool的子类有ExecutionMemoryPool和StorageMemoryPool，分别用于execution </p>
<p>memory 和 storage memory的内存池</p>
</li>
<li><p>从内存的位置来划分（MemoryAllocator）；</p>
<p>ON_HEAP memory and OFF_HEAP memory，所以MemoryAllocator的实现类有HeapMemoryAllocator和UnsafeMemoryAllocator</p>
</li>
</ol>
<h2 id="MemoryManager的实现"><a href="#MemoryManager的实现" class="headerlink" title="MemoryManager的实现"></a>MemoryManager的实现</h2><p>UnifiedMemoryManager是目前Spark中唯一的MemoryManager的实现，之前的静态内存管理方案在Spark 1.6中被废弃了。</p>
<p>MemoryManager定义了主要的内存管理接口</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def acquireStorageMemory(blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean</span><br><span class="line"></span><br><span class="line">def acquireUnrollMemory(blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean</span><br><span class="line"></span><br><span class="line">def acquireExecutionMemory(numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long</span><br><span class="line"></span><br><span class="line">def releaseStorageMemory(numBytes: Long, memoryMode: MemoryMode): Unit</span><br><span class="line"></span><br><span class="line">def releaseExecutionMemory(numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Unit</span><br><span class="line"></span><br><span class="line">def releaseUnrollMemory(numBytes: Long, memoryMode: MemoryMode): Unit</span><br></pre></td></tr></table></figure>
<p>UnifiedMemoryManager机制</p>
<ul>
<li>设定基本的存储内存和执行内存区域（spark.storage.storageFraction 参数），该设定确定了双方各自拥有的空间的范围</li>
<li>双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block）</li>
<li>执行内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间</li>
<li>存储内存的空间被对方占用后，无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂</li>
</ul>
<p><img src="/images/spark/image001.png"></p>
<h2 id="Spark中的内存使用"><a href="#Spark中的内存使用" class="headerlink" title="Spark中的内存使用"></a>Spark中的内存使用</h2><p>在前面Spark内存的分类中，已经了解到Spark的内存按照作用来分，可以分为Execution memory and Storage memory，依次分析如下：</p>
<h3 id="Execution-memory"><a href="#Execution-memory" class="headerlink" title="Execution memory"></a>Execution memory</h3><p>来自代码的注释：</p>
<p><em>execution memory refers to that used for computation in shuffles, joins, sorts and aggregations</em></p>
<p>Execution memory简单点说就是用于Task自身执行过程中使用到的内存和用于shuffle数据的内存。</p>
<h4 id="Task执行内存"><a href="#Task执行内存" class="headerlink" title="Task执行内存"></a>Task执行内存</h4><p>来自代码的注释：</p>
<p>“<em>Tries to ensure that each task gets a reasonable share of memory, instead of some task ramping up to a large amount first and then causing others to spill to disk repeatedly.</em><br><em>If there are N tasks, it ensures that each task can acquire at least 1 / 2N of the memory before it has to spill, and at most 1 / N. Because N varies dynamically, we keep track of the set of active tasks and redo the calculations of 1 / 2N and 1 / N in waiting tasks whenever this set changes. This is all done by synchronizing access to mutable state and using wait() and notifyAll() to signal changes to callers. Prior to Spark 1.6, this arbitration of memory across tasks was performed by the ShuffleMemoryManager.</em>“</p>
<p>Executor 内运行的任务同样共享执行内存，Spark 用一个 HashMap 结构保存了任务到内存耗费的映射。每个任务可占用的执行内存大小的范围为 1/2N ~ 1/N，其中 N 为当前 Executor 内正在运行的任务的个数。每个任务在启动之时，要向 MemoryManager 请求申请最少为 1/2N 的执行内存，如果不能被满足要求则该任务被阻塞，直到有其他任务释放了足够的执行内存，该任务才可以被唤醒。这块儿的内存管理由TaskMemoryManager管理</p>
<h4 id="Shuffle内存"><a href="#Shuffle内存" class="headerlink" title="Shuffle内存"></a>Shuffle内存</h4><p>关于Shuffle内存这块儿的逻辑，可参考后续Shuffle模块儿的分析</p>
<h3 id="Storage-memory"><a href="#Storage-memory" class="headerlink" title="Storage memory"></a>Storage memory</h3><p>来自代码的注释：</p>
<p><em>storage memory refers to that used for caching and propagating internal data across the cluster</em></p>
<p>Storage memory的设计，可以对缓存 RDD 时使用的内存做统一的规划和管理同时作用于缓存 broadcast 数据。</p>
<p>RDD的缓存管理由Spark的Storage 模块儿负责实现，其中缓存的级别不仅仅包含内存，还包括Disk、远端存储等，同时还有缓存策略以及淘汰策略等逻辑。可以参考后续博客了解。</p>
<h2 id="Spark中内存Mode"><a href="#Spark中内存Mode" class="headerlink" title="Spark中内存Mode"></a>Spark中内存Mode</h2><p>在前面Spark内存的分类中，已经了解到Spark的内存位置来分可以分为：ON_HEAP memory and OFF_HEAP memory</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public enum MemoryMode &#123;</span><br><span class="line">  ON_HEAP,</span><br><span class="line">  OFF_HEAP</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其实就是两种MemoryAllocator</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MemoryAllocator UNSAFE &#x3D; new UnsafeMemoryAllocator();</span><br><span class="line">MemoryAllocator HEAP &#x3D; new HeapMemoryAllocator();</span><br></pre></td></tr></table></figure>
<p>MemoryAllocator的接口如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MemoryBlock allocate(long size) throws OutOfMemoryError;</span><br><span class="line">void free(MemoryBlock memory);</span><br></pre></td></tr></table></figure>
<h3 id="堆内内存"><a href="#堆内内存" class="headerlink" title="堆内内存"></a>堆内内存</h3><p>堆内内存其实就是使用HeapMemoryAllocator进行分配的内存</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * A simple &#123;@link MemoryAllocator&#125; that can allocate up to 16GB using a JVM long</span><br><span class="line"> * primitive array.</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class HeapMemoryAllocator implements MemoryAllocator &#123;</span><br><span class="line"></span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  private final Map&lt;Long, LinkedList&lt;WeakReference&lt;long[]&gt;&gt;&gt; bufferPoolsBySize &#x3D; new </span><br><span class="line">                                                                        HashMap&lt;&gt;();</span><br><span class="line">  ......  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/images/spark/image002.png"></p>
<p>Spark 对堆内内存的管理是一种逻辑上的”规划式”的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前 记录 这些内存。Spark是不能精准控制堆内内存的申请和释放，所以不能完全避免内存溢出，只是一定程度的减少了异常的出现。</p>
<h3 id="堆外内存"><a href="#堆外内存" class="headerlink" title="堆外内存"></a>堆外内存</h3><p>堆内内存其实就是使用UnsafeMemoryAllocator进行分配的内存</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class UnsafeMemoryAllocator implements MemoryAllocator &#123;</span><br><span class="line">  @Override</span><br><span class="line">  public MemoryBlock allocate(long size) throws OutOfMemoryError &#123;</span><br><span class="line">    long address &#x3D; Platform.allocateMemory(size);</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">  @Override</span><br><span class="line">  public void free(MemoryBlock memory) &#123;</span><br><span class="line">    ...</span><br><span class="line">    Platform.freeMemory(memory.offset);</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其核心是调用Platform中JDK Unsafe API，关于Platform的实现可参考后续博客</p>
<p><img src="/images/spark/image003.png"></p>
<p>Spark通过JDK Unsafe API实现了堆外内存的管理, 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。<a href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html">具体参考tungsten计划</a></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文从MemoryManager（根入口）结合 UnifiedMemoryManager（MemoryManager的唯一实现）的逻辑，从内存位置方面，分析了（ON_HEAP和OFF_HEAP），从内存使用方面，分析了（Execution memory and Storage memory）。</p>
<p>最核心的当然是MemoryManager，跟MemoryManager的交互却是TaskMemoryManager。</p>
<p>来自代码的注释：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">                                                       +---------------------------+</span><br><span class="line">+-------------+                                        |       MemoryManager       |</span><br><span class="line">| MemConsumer |----+                                   |                           |</span><br><span class="line">+-------------+    |    +-------------------+          |  +---------------------+  |</span><br><span class="line">                   +---&gt;| TaskMemoryManager |----+     |  |OnHeapStorageMemPool |  |</span><br><span class="line">+-------------+    |    +-------------------+    |     |  +---------------------+  |</span><br><span class="line">| MemConsumer |----+                             |     |                           |</span><br><span class="line">+-------------+         +-------------------+    |     |  +---------------------+  |</span><br><span class="line">                        | TaskMemoryManager |----+     |  |OffHeapStorageMemPool|  |</span><br><span class="line">                        +-------------------+    |     |  +---------------------+  |</span><br><span class="line">                                                 +----&gt;|                           |</span><br><span class="line">                                 *               |     |  +---------------------+  |</span><br><span class="line">                                 *               |     |  |OnHeapExecMemPool    |  |</span><br><span class="line">+-------------+                  *               |     |  +---------------------+  |</span><br><span class="line">| MemConsumer |----+                             |     |                           |</span><br><span class="line">+-------------+    |    +-------------------+    |     |  +---------------------+  |</span><br><span class="line">                   +---&gt;| TaskMemoryManager |----+     |  |OffHeapExecMemPool   |  |</span><br><span class="line">                        +-------------------+          |  +---------------------+  |</span><br><span class="line">                                                       |                           |</span><br><span class="line">                                                       +---------------------------+</span><br></pre></td></tr></table></figure>


<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
<p><a href="https://developer.ibm.com/zh/articles/ba-cn-apache-spark-memory-management/">Apache Spark 内存管理详解</a></p>
]]></content>
      <categories>
        <category>Spark基础系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark DAGScheduler</title>
    <url>/2021/03/23/Spark/Spark%E5%9F%BA%E7%A1%80-8/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在Spark中有一个核心模块调度器（Scheduler），这个核心模块Scheduler是在SparkContext的创建过程中完成初始化。这个Scheduler是面向应用的调度，跟ClusterManager面向资源的调度是由本质的区别的。</p>
<a id="more"></a>

<p>Scheduler由两个Level的调度</p>
<ul>
<li>DAGScheduler，一个面向Stage的调度，主要功能包括Stage的生成、调度。</li>
<li>TaskScheduler，一个面向Task的调度，是一个低级别的调度器。</li>
</ul>
<p>在Spark中，存在这样的转化关系</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Application --(1:n)--&gt;  Job --(1:n)--&gt; Stage --(1:n)--&gt; Task</span><br><span class="line">                                         ^               ^</span><br><span class="line">                                         |               |</span><br><span class="line">                                      DAGScheduler   TaskScheduler</span><br></pre></td></tr></table></figure>
<p>源码目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;home&#x2F;penglei&#x2F;Bigdata&#x2F;spark&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;DAGScheduler.scala</span><br></pre></td></tr></table></figure>
<p>在SparkContext中创建DAGScheduler</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">_dagScheduler &#x3D; new DAGScheduler(this)</span><br></pre></td></tr></table></figure>


<h2 id="核心功能"><a href="#核心功能" class="headerlink" title="核心功能"></a>核心功能</h2><p> <em>“Jobs (represented by [[ActiveJob]]) are the top-level work items submitted to the scheduler. For example, when the user calls an action, like count(), a job will be submitted through submitJob. Each Job may require the execution of multiple stages to build intermediate data.“</em></p>
<p>Jobs作业的管理，每一个action算子产生一个Job，一个SparkSession对应一个SparkContext，一个SparkContext对应一个DAGScheduler，如果Application中有多个action算子的话，会有多个Job产生，即一个DAGScheduler会处理所有Application的Jobs。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private[scheduler] val activeJobs &#x3D; new HashSet[ActiveJob]</span><br></pre></td></tr></table></figure>


<p> <em>”Stages ([[Stage]]) are sets of tasks that compute intermediate results in jobs, where each task computes the same function on partitions of the same RDD. Stages are separated at shuffle boundaries, which introduce a barrier (where we must wait for the previous stage to finish to fetch outputs). There are two types of stages: [[ResultStage]], for the final stage that executes an action, and [[ShuffleMapStage]], which writes map output files for a shuffle. Stages are often shared across multiple jobs, if these jobs reuse the same RDDs.“</em></p>
<p>Stage有两种</p>
<ul>
<li>ResultStage</li>
<li>ShuffleMapStage</li>
</ul>
<p>Stage的切分（宽窄依赖关系）和调度以及多Jobs间Stage的共享，都是由DAGScheduler来处理。</p>
<p> <em>”Tasks are individual units of work, each sent to one machine.“</em></p>
<p>DAGScheduler会为Stage创建对应的TaskSet，然后传给TaskScheduler进行处理。</p>
<p> <em>”Cache tracking: the DAGScheduler figures out which RDDs are cached to avoid recomputing them and likewise remembers which shuffle map stages have already produced output files to avoid redoing the map side of a shuffle.“</em></p>
<p>DAGScheduler能够对RDD的cache进行跟踪，避免重复计算，同样shuffle map stage的输出也会尽量重用，避免重复计算。</p>
<p> <em>”Preferred locations: the DAGScheduler also computes where to run each task in a stage based on the preferred locations of its underlying RDDs, or the location of cached or shuffle data.“</em></p>
<p>亲和性调度，计算和存储尽量放一起。</p>
<p> <em>”Cleanup: all data structures are cleared when the running jobs that depend on them finish, to prevent memory leaks in a long-running application.“</em></p>
<p>资源清理包括缓存和临时数据，避免资源的泄漏</p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>DAGScheduler采用的事件处理机制</p>
<ul>
<li><p>eventProcessLoop</p>
<p>事件接收处理的服务端，接收事件并根据事件的不同的类型做出相应的处理。</p>
</li>
<li><p>messageScheduler</p>
<p>事件的客户端（生产），入参是一个Runnable对象，发送事件给服务端。</p>
</li>
</ul>
<p>简单来说就是一个生产者消费者模型，通过messageScheduler发送事件消息给DAGScheduler，比如Job的完成、Task的一些状态和异常等。DAGScheduler通过eventProcessLoop不停的接收消息处理消息。当然了生产者不仅仅是messageScheduler，还可以绕过messageScheduler，直接使用eventProcessLoop往其queue中放入事件消息。因为messageScheduler传入的Runnable对象其实也是调用eventProcessLoop的方法,区别是messageScheduler可以周期性一直运行产生事件消息。</p>
<p>在DAGScheduler中还有个listenerBus，listenerBus跟messageScheduler的区别是listenerBus把消息发送到了消息总线上，而messageScheduler仅仅是发送给了DAGScheduler。</p>
<h3 id="eventProcessLoop"><a href="#eventProcessLoop" class="headerlink" title="eventProcessLoop"></a>eventProcessLoop</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private[spark] val eventProcessLoop &#x3D; new DAGSchedulerEventProcessLoop(this)</span><br></pre></td></tr></table></figure>
<p>在DAGScheduler的构造函数最后，启动了eventProcessLoop线程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">step1:启动eventProcessLoop</span><br><span class="line">eventProcessLoop.start()</span><br><span class="line"></span><br><span class="line">step2:调用Thread的start方法，进一步调用Tread的run方法</span><br><span class="line">def start(): Unit &#x3D; &#123;</span><br><span class="line">    ...</span><br><span class="line">    eventThread.start()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">step3:开始监听事件</span><br><span class="line">  private[spark] val eventThread &#x3D; new Thread(name) &#123;</span><br><span class="line">    setDaemon(true)</span><br><span class="line"></span><br><span class="line">    override def run(): Unit &#x3D; &#123;</span><br><span class="line">      ...</span><br><span class="line">      onReceive(event)</span><br><span class="line">      ...</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>DAGSchedulerEventProcessLoop的事件处理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * The main event loop of the DAG scheduler.</span><br><span class="line"> *&#x2F;</span><br><span class="line">override def onReceive(event: DAGSchedulerEvent): Unit &#x3D; &#123;</span><br><span class="line">  val timerContext &#x3D; timer.time()</span><br><span class="line">  try &#123;</span><br><span class="line">    doOnReceive(event)</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    timerContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>doOnReceive方法：</p>
<ul>
<li>JobSubmitted：提交Job事件</li>
<li>MapStageSubmitted：Map Stage提交事件</li>
<li>StageCancelled：Stage取消事件</li>
<li>JobCancelled：Job取消事件</li>
<li>JobGroupCancelled：Job Group取消事件</li>
<li>AllJobsCancelled：取消所有Job的事件</li>
<li>ExecutorAdded：添加Executor的事件</li>
<li>ExecutorLost：Executor失联事件</li>
<li>WorkerRemoved：Worker移除事件</li>
<li>BeginEvent： Task开始执行的事件</li>
<li>SpeculativeTaskSubmitted： </li>
<li>UnschedulableTaskSetAdded：在动态资源分配的特性下，添加调度的TaskSet</li>
<li>UnschedulableTaskSetRemoved：在动态资源分配的特性下，移除调度的TaskSet</li>
<li>GettingResultEvent： 获取Result事件</li>
<li>CompletionEvent：完成事件</li>
<li>TaskSetFailed：TaskSet失败事件</li>
<li>ResubmitFailedStages：重新提交失败的Stage的事件</li>
</ul>
<h3 id="messageScheduler"><a href="#messageScheduler" class="headerlink" title="messageScheduler"></a>messageScheduler</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private val messageScheduler &#x3D;</span><br><span class="line">    ThreadUtils.newDaemonSingleThreadScheduledExecutor(&quot;dag-scheduler-message&quot;)</span><br></pre></td></tr></table></figure>
<p>example</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">eg—1：</span><br><span class="line">messageScheduler.schedule(</span><br><span class="line">            new Runnable &#123;</span><br><span class="line">              override def run(): Unit &#x3D; eventProcessLoop.post(JobSubmitted(jobId, finalRDD, func,</span><br><span class="line">                partitions, callSite, listener, properties))</span><br><span class="line">            &#125;,</span><br><span class="line">            timeIntervalNumTasksCheck,</span><br><span class="line">            TimeUnit.SECONDS</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">eg-2:</span><br><span class="line">messageScheduler.schedule(</span><br><span class="line">            new Runnable &#123;</span><br><span class="line">              override def run(): Unit &#x3D; eventProcessLoop.post(ResubmitFailedStages)</span><br><span class="line">            &#125;,</span><br><span class="line">            DAGScheduler.RESUBMIT_TIMEOUT,</span><br><span class="line">            TimeUnit.MILLISECONDS</span><br><span class="line">)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>DAGScheduler采用的事件处理机制，完成其各个功能包括stage的切分和调度，以及Driver和Executor之间的各种事件消息的通信等。这仅仅是整个DAGScheduler的框架，DAGScheduler真正的核心是各种handle方法，包括handleJobSubmitted、handleTaskCompletion等。</p>
<h2 id="核心Handle方法"><a href="#核心Handle方法" class="headerlink" title="核心Handle方法"></a>核心Handle方法</h2><h3 id="handleJobSubmitted"><a href="#handleJobSubmitted" class="headerlink" title="handleJobSubmitted"></a>handleJobSubmitted</h3><h3 id="handleTaskCompletion"><a href="#handleTaskCompletion" class="headerlink" title="handleTaskCompletion"></a>handleTaskCompletion</h3><h3 id="others"><a href="#others" class="headerlink" title="others"></a>others</h3><p>Toto</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
]]></content>
      <categories>
        <category>Spark基础系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark TaskScheduler</title>
    <url>/2021/03/26/Spark/Spark%E5%9F%BA%E7%A1%80-9/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在Spark中有一个核心模块调度器（Scheduler），这个核心模块Scheduler是在SparkContext的创建过程中完成初始化。这个Scheduler是面向应用的调度，跟ClusterManager面向资源的调度是由本质的区别的。</p>
<a id="more"></a>



<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
]]></content>
      <categories>
        <category>Spark基础系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark源码分析系列</title>
    <url>/2021/03/23/Spark/Spark%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<p>Spark源码分析系列是自己在学习Spark过程中的总结和记录，基于Spark3+版本，一共分为三个子系列</p>
<a id="more"></a>

<ul>
<li>Spark基础系列</li>
<li>Spark On K8S系列</li>
<li>Spark SQL系列</li>
</ul>
<p><a href="http://www.mobanche.top/2021/03/25/Spark/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%8E%AF%E5%A2%83/">Spark源码阅读和调试</a></p>
<h2 id="Spark基础系列"><a href="#Spark基础系列" class="headerlink" title="Spark基础系列"></a>Spark基础系列</h2><ul>
<li><a href="http://www.mobanche.top/2021/03/23/Spark/Spark%E5%9F%BA%E7%A1%80-1/">1</a></li>
<li><a href="http://www.mobanche.top/2021/03/23/Spark/Spark%E5%9F%BA%E7%A1%80-2/">2</a></li>
<li><a href="http://www.mobanche.top/2021/03/23/Spark/Spark%E5%9F%BA%E7%A1%80-3/">3</a></li>
<li><a href="http://www.mobanche.top/2021/03/23/Spark/Spark%E5%9F%BA%E7%A1%80-4/">4</a></li>
<li><a href="http://www.mobanche.top/2021/03/23/Spark/Spark%E5%9F%BA%E7%A1%80-5/">5</a></li>
<li><a href="http://www.mobanche.top/2021/03/23/Spark/Spark%E5%9F%BA%E7%A1%80-6/">6</a></li>
<li><a href="http://www.mobanche.top/2021/03/23/Spark/Spark%E5%9F%BA%E7%A1%80-7/">7</a></li>
<li><a href="http://www.mobanche.top/2021/03/23/Spark/Spark%E5%9F%BA%E7%A1%80-8/">8</a></li>
</ul>
<h2 id="Spark-On-K8S系列"><a href="#Spark-On-K8S系列" class="headerlink" title="Spark On K8S系列"></a>Spark On K8S系列</h2><ul>
<li><a href="http://www.mobanche.top/2021/03/23/Spark/Spark-On-K8S-1/">Spark on K8S</a></li>
<li><a href="http://www.mobanche.top/2021/03/23/Spark/Spark-On-K8S-2/">Spark on K8S Submit分析</a></li>
<li><a href="http://www.mobanche.top/2021/03/23/Spark/Spark-On-K8S-3/">Spark on K8S Feature分析</a></li>
<li><a href="http://www.mobanche.top/2021/03/23/Spark/Spark-On-K8S-4/">Spark on K8S Scheduler分析</a></li>
<li><a href="http://www.mobanche.top/2021/03/23/Spark/Spark-On-K8S-5/">Spark on K8S Shuffle分析</a></li>
<li><a href="http://www.mobanche.top/2021/03/23/Spark/Spark-On-K8S-6/">Spark on K8S 运行流程</a></li>
<li><a href="http://www.mobanche.top/2021/03/23/Spark/Spark-On-K8S-7/">Spark on K8S Demo尝试</a></li>
</ul>
<h2 id="Spark-SQL系列"><a href="#Spark-SQL系列" class="headerlink" title="Spark SQL系列"></a>Spark SQL系列</h2>]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark源码阅读环境</title>
    <url>/2021/03/25/Spark/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<p>如何方便的阅读Spark源码以及如何运行调试Example其实是一件比较简单的事情，但是对初学Spark的人来说还是比较复杂，成功的配置好Spark源码阅读环境可以更方便的学习Spark原理。</p>
<a id="more"></a>

<p>本文主要介绍如何在Windows上阅读调试Spark代码, Spark代码版本3+。</p>
<h2 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h2><ul>
<li>JDK: 1.8 （Windows）</li>
<li>MAVEN: 3.6.3（Windows）</li>
<li>SCALA: 2.12.13（Windows）</li>
<li>IDEA: 2020.3（Windows端,需要安装Scala插件）</li>
<li>WSL2【Ubuntu： 20.04】 （辅助，代码并不会放到WSL2的子系统上）</li>
</ul>
<h2 id="导入IDEA"><a href="#导入IDEA" class="headerlink" title="导入IDEA"></a>导入IDEA</h2><ol>
<li><p>导入</p>
<p>先配置一下maven</p>
<p><img src="/images/spark/image016.png"></p>
<p>接着导入maven工程</p>
<p><img src="/images/spark/image017.png"></p>
<p><img src="/images/spark/image018.png"></p>
</li>
<li><p>编译</p>
<p>这里建议是使用mvn compile，而不是使用idea的build选项</p>
<p><img src="/images/spark/image020.png"></p>
<p>这样编译完之后代码的跳转不会有任何问题，包括spark-catalyst</p>
<p><img src="/images/spark/image021.png"></p>
<p>整个编译过程如果按着步骤来，肯定不会出现任何问题，因为在idea中，我们使用的是mvn编译，本质上跟git bash或者wsl里执行mvn编译没有区别。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">E:\App\Java\Jdk\bin\java.exe </span><br><span class="line">-Dmaven.multiModuleProjectDirectory&#x3D;E:\Code\spark </span><br><span class="line">-Dmaven.home&#x3D;E:\App\Maven\apache-maven-3.6.3-bin\apache-maven-3.6.3 </span><br><span class="line">-Dclassworlds.conf&#x3D;E:\App\Maven\apache-maven-3.6.3-bin\apache-maven-3.6.3\bin\m2.conf </span><br><span class="line">&quot;-Dmaven.ext.class.path&#x3D;E:\App\IntelliJ IDEA 2020.3.3\plugins\maven\lib\maven-event-listener.jar&quot; </span><br><span class="line">&quot;-javaagent:E:\App\IntelliJ IDEA 2020.3.3\lib\idea_rt.jar&#x3D;57479:E:\App\IntelliJ IDEA 2020.3.3\bin&quot; </span><br><span class="line">-Dfile.encoding&#x3D;UTF-8 </span><br><span class="line">-classpath </span><br><span class="line">E:\App\Maven\apache-maven-3.6.3-bin\apache-maven-3.6.3\boot\plexus-classworlds-2.6.0.jar;E:\App\Maven\apache-maven-3.6.3-bin\apache-maven-3.6.3\boot\plexus-classworlds.license org.codehaus.classworlds.Launcher </span><br><span class="line">-Didea.version&#x3D;2020.3.3 </span><br><span class="line">-s E:\App\Maven\apache-maven-3.6.3-bin\apache-maven-3.6.3\conf\settings.xml </span><br><span class="line">-Dmaven.repo.local&#x3D;E:\App\Maven\repository </span><br><span class="line"></span><br><span class="line">###这就是在idea中执行mvn package得到的命令</span><br><span class="line">-DskipTests&#x3D;true package</span><br></pre></td></tr></table></figure></li>
<li><p>打包</p>
<p>如果想要在idea中执行example或者调试，那么必须要有jar包，可以参考maven的各个生命周期，此时我们需要执行mvn package。</p>
<p><img src="/images/spark/image019.png"></p>
</li>
</ol>
<h2 id="运行example"><a href="#运行example" class="headerlink" title="运行example"></a>运行example</h2><p>我们以SparkPi为例</p>
<p><img src="/images/spark/image025.png"></p>
<p>在配置中可以配置一下是否build，这个看个人需求，我是去掉了。</p>
<p>执行run SparkPi的时候，会发生两个主要错误：</p>
<ul>
<li><p>java.lang.ClassNotFoundException: scala.collection.Seq</p>
<p><img src="/images/spark/image022.png"></p>
<p>解决办法：</p>
<p><img src="/images/spark/image023.png"></p>
</li>
<li><p>Could not find spark-version-info.properties</p>
<p><img src="/images/spark/image024.png"></p>
<p>原因是我们没有使用./build/mvn去编译Spark源码.但是没关系，生成spark-version-info.properties就好了</p>
<p>解决办法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;build&#x2F;spark-build-info .&#x2F;core&#x2F;target&#x2F;scala-2.12&#x2F;classes&#x2F; 3.2.0-SNAPSHOT</span><br></pre></td></tr></table></figure>
<p>​       </p>
</li>
</ul>
<p>至此我们可以正常运行SparkPi这个Example了，当然也可以debug去看流程。</p>
<p><img src="/images/spark/image026.png"></p>
<p>这里有一个小的瑕疵就是hadoop相关的配置没有，但是如果你的应用程序或者运行的Example没有使用到hadoop的相关组件的话，其实是不用理会的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https:&#x2F;&#x2F;wiki.apache.org&#x2F;hadoop&#x2F;WindowsProblems</span><br><span class="line">	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)</span><br><span class="line">	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)</span><br><span class="line">	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)</span><br><span class="line">	at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:689)</span><br><span class="line">	at org.apache.hadoop.util.StringUtils.&lt;clinit&gt;(StringUtils.java:78)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)</span><br><span class="line">	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)</span><br><span class="line">	at org.apache.hadoop.security.SecurityUtil.&lt;clinit&gt;(SecurityUtil.java:88)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1860)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:718)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:668)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:579)</span><br><span class="line">	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2485)</span><br><span class="line">	at scala.Option.getOrElse(Option.scala:189)</span><br><span class="line">	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2485)</span><br><span class="line">	at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:314)</span><br><span class="line">	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2696)</span><br><span class="line">	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:948)</span><br><span class="line">	at scala.Option.getOrElse(Option.scala:189)</span><br><span class="line">	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:942)</span><br><span class="line">	at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:30)</span><br><span class="line">	at org.apache.spark.examples.SparkPi.main(SparkPi.scala)</span><br><span class="line">Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.</span><br><span class="line">	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)</span><br><span class="line">	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)</span><br><span class="line">	at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:516)</span><br><span class="line">	... 20 more</span><br></pre></td></tr></table></figure>
<p>如果想使用hadoop的相关组件的话，建议在WSL2中安装hadoop，可以参考这个<a href="https://dev.to/samujjwaal/hadoop-installation-on-windows-10-using-wsl-2ck1">链接</a>，其实有一个坑</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">配置完成后</span><br><span class="line">ssh localhost还是不可以</span><br><span class="line">参考https:&#x2F;&#x2F;unix.stackexchange.com&#x2F;questions&#x2F;487742&#x2F;system-is-booting-up-unprivileged-users-are-not-permitted-to-log-in-yet 来解决</span><br></pre></td></tr></table></figure>
<p>总的来说，这里我们spark.master=local，对于学习流程来说已经足够了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文总结了Spark源码导入IDEA的过程、以及如何使用IDEA去编译、打包Spark源码，同时以SparkPi为例，展示了如何运行和Debug Spark Application，这将很好的帮助我们学习Spark源码。毕竟我们的桌面办公环境大多数都是Windows平台。Just enjoy it。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>WSL2作为Docker后端</title>
    <url>/2021/03/23/WSL2/WSL2%E4%BD%9C%E4%B8%BADocker%E5%90%8E%E7%AB%AF/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Windows10安装docker的方式有两种: </p>
<ul>
<li>原生的Linux安装docker，即在WSL2上安装docker，因为WSL2本质上就是一个Linux内核</li>
<li>Docker Desktop for windows方式</li>
</ul>
<a id="more"></a>

<p>在WSL2下原生Linux安装docker的方式跟完全的Linux虚拟机安装docker类似，区别在于WSL2的Linux不支持systemd，但是可以通过使用官方的get-docker.sh脚本进行安装docker，这样dockerd进程使用ubuntu的init方式启动而不是systemd方式启动。</p>
<p>本文主要介绍一下Docker Desktop for windows方式</p>
<h2 id="Docker-Desktop-for-windows方式"><a href="#Docker-Desktop-for-windows方式" class="headerlink" title="Docker Desktop for windows方式"></a>Docker Desktop for windows方式</h2><h3 id="WSL2的安装和配置"><a href="#WSL2的安装和配置" class="headerlink" title="WSL2的安装和配置"></a>WSL2的安装和配置</h3><p>这个网上很多博客包括微软的doc都有介绍</p>
<p>重点是配置WSL2的systemd功能，可参考之前的博客</p>
<h3 id="Docker安装"><a href="#Docker安装" class="headerlink" title="Docker安装"></a>Docker安装</h3><ol>
<li><p>下载Docker Desktop并安装</p>
<p><a href="https://docs.docker.com/docker-for-windows/wsl/">https://docs.docker.com/docker-for-windows/wsl/</a></p>
</li>
<li><p>配置</p>
</li>
</ol>
<p>​      <img src="/images/others/image001.png"></p>
<p>​      <img src="/images/others/image002.png"></p>
<p>其实安装挺简单，最后确认一把安装结果即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">penglei@PengLei:~$ docker version</span><br><span class="line">Client: Docker Engine - Community</span><br><span class="line"> Cloud integration: 1.0.7</span><br><span class="line"> Version:           20.10.2</span><br><span class="line"> API version:       1.41</span><br><span class="line"> Go version:        go1.13.15</span><br><span class="line"> Git commit:        2291f61</span><br><span class="line"> Built:             Mon Dec 28 16:17:34 2020</span><br><span class="line"> OS&#x2F;Arch:           linux&#x2F;amd64</span><br><span class="line"> Context:           default</span><br><span class="line"> Experimental:      true</span><br><span class="line"></span><br><span class="line">Server: Docker Engine - Community</span><br><span class="line"> Engine:</span><br><span class="line">  Version:          20.10.2</span><br><span class="line">  API version:      1.41 (minimum version 1.12)</span><br><span class="line">  Go version:       go1.13.15</span><br><span class="line">  Git commit:       8891c58</span><br><span class="line">  Built:            Mon Dec 28 16:15:28 2020</span><br><span class="line">  OS&#x2F;Arch:          linux&#x2F;amd64</span><br><span class="line">  Experimental:     false</span><br><span class="line"> containerd:</span><br><span class="line">  Version:          1.4.3</span><br><span class="line">  GitCommit:        269548fa27e0089a8b8278fc4fc781d7f65a939b</span><br><span class="line"> runc:</span><br><span class="line">  Version:          1.0.0-rc92</span><br><span class="line">  GitCommit:        ff819c7e9184c13b7c2607fe6c30ae19403a7aff</span><br><span class="line"> docker-init:</span><br><span class="line">  Version:          0.19.0</span><br><span class="line">  GitCommit:        de40ad0</span><br><span class="line">penglei@PengLei:~$</span><br></pre></td></tr></table></figure>
<h3 id="为什么选择这种方式"><a href="#为什么选择这种方式" class="headerlink" title="为什么选择这种方式"></a>为什么选择这种方式</h3><p>WSL2原生的Linux内核，完全可以安装Docker了，只需要配置systemd功能，或者使用官方的get-docker.sh的方式就行了，这样不香吗？嗯是挺香的</p>
<p>但是我喜欢使用WSL2啊，我在Windows上安装我喜欢的IDE，然后通过Remote-WSL，连接WSL很香啊，coding、debug、test</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://docs.docker.com/docker-for-windows/wsl/">WSL2作为Docker后端</a></p>
]]></content>
      <categories>
        <category>Others</category>
      </categories>
      <tags>
        <tag>WSL2</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>WSL2添加Systemd功能</title>
    <url>/2021/03/23/WSL2/WSL2%E6%B7%BB%E5%8A%A0Systemd%E5%8A%9F%E8%83%BD/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>WSL2默认是没有systemd服务的，需要自行去安装。</p>
<a id="more"></a>

<p>Systemd 是 Linux 系统工具，为系统的启动和管理提供一套完整的解决方案。已成为大多数发行版的标准配置。</p>
<p>根据 Linux 惯例，字母<code>d</code>是守护进程（daemon）的缩写。 Systemd 这个名字的含义，就是它要守护整个系统。</p>
<p>Systemd 并不是一个命令，而是一组命令，涉及到系统管理的方方面面。</p>
<h2 id="WLS2安装systemd"><a href="#WLS2安装systemd" class="headerlink" title="WLS2安装systemd"></a>WLS2安装systemd</h2><h3 id="执行命令"><a href="#执行命令" class="headerlink" title="执行命令"></a>执行命令</h3><p><a href="https://github.com/DamionGans/ubuntu-wsl2-systemd-script">https://github.com/DamionGans/ubuntu-wsl2-systemd-script</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;DamionGans&#x2F;ubuntu-wsl2-systemd-script.git</span><br><span class="line">cd ubuntu-wsl2-systemd-script&#x2F;</span><br><span class="line">bash ubuntu-wsl2-systemd-script.sh</span><br><span class="line"># Enter your password and wait until the script has finished</span><br></pre></td></tr></table></figure>
<h3 id="验证安装结果"><a href="#验证安装结果" class="headerlink" title="验证安装结果"></a>验证安装结果</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">penglei@PengLei:~$ systemctl --version</span><br><span class="line">systemd 245 (245.4-4ubuntu3.2)</span><br><span class="line">+PAM +AUDIT +SELINUX +IMA +APPARMOR +SMACK +SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT +GNUTLS +ACL +XZ +LZ4 +SECCOMP +BLKID +ELFUTILS +KMOD +IDN2 -IDN +PCRE2 default-hierarchy&#x3D;hybrid</span><br><span class="line">penglei@PengLei:~$</span><br></pre></td></tr></table></figure>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>网上有很多文章都介绍了WSL2支持systemd的操作指导，但我觉得ubuntu-wsl2-systemd-script还是非常简单好用，基本上避免了自行安装的很多坑</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html">Systemd</a></p>
]]></content>
      <categories>
        <category>Others</category>
      </categories>
      <tags>
        <tag>WSL2</tag>
        <tag>systemctl</tag>
      </tags>
  </entry>
  <entry>
    <title>Support ANSI SQL INTERVAL TYPES[SPARK-27790]</title>
    <url>/2021/05/19/Spark/Spark-SQL-01/</url>
    <content><![CDATA[<p><a href="https://issues.apache.org/jira/browse/SPARK-27790">SPARK-27790</a> Support ANSI SQL INTERVAL Types</p>
<p>本文主要对Spark SQL中添加ANSI SQL INTERVAL 类型的背景、开发过程、代码做了简单的梳理总结，目前分析还不够深入，因为我只是学习了各个PR的代码。相比每个PR的代码来说，PR的评论、Review意见是更值得深入思考的地方，这个还是需要一定的时间消化。</p>
<p>未完待续，持续更新</p>
<a id="more"></a>

<p>其实Spark SQL中是有INTERVAL的数据类型的，但是其实是不好用的：</p>
<ul>
<li>已有的INTERVAL数据类型不支持持久化</li>
<li>已有的INTERVAL数据类型不能够比较，举个例子“1 Month 1 Day” is equal to “1 Month 1 Day” ？，由于一个月到底是30天呢，还是31天，更或者是28/29天，都不确定，导致了已有的INTERVAL数据类型无法进行比较</li>
</ul>
<p>介于上面的背景，Spark社区引入两种符合ANSI SQL 标准的INTERVAL的数据类型来替代现有的INTERVAL数据类型：</p>
<ul>
<li>YEAR-MONTH/DAY-SECOND </li>
<li>两种类型中的字段是可以比较可排序</li>
<li>两种类型支持日期类型的计算</li>
<li>可以持久化</li>
</ul>
<p>两种风格的INTERVAL数据类型将在一段时间内并存。</p>
<p>目前<a href="https://issues.apache.org/jira/browse/SPARK-27790">SPARK-27790</a>还在开发中，Spark3.2版本Milestone1就会完成.</p>
<p>相关SubTask汇总如下：</p>
<ul>
<li><p>类型定义和外部输入类型：<a href="https://issues.apache.org/jira/browse/SPARK-27793">SPARK-27793</a> <a href="https://issues.apache.org/jira/browse/SPARK-34605">SPARK-34605</a> <a href="https://issues.apache.org/jira/browse/SPARK-34615">SPARK-34615</a> </p>
</li>
<li><p>expressions处理</p>
<ul>
<li>Cast：<a href="https://issues.apache.org/jira/browse/SPARK-34667">SPARK-34667</a> <a href="https://issues.apache.org/jira/browse/SPARK-34668">SPARK-34668</a></li>
<li>UnaryMinus/Add/Subtract:  <a href="https://issues.apache.org/jira/browse/SPARK-34677">SPARK-34677</a></li>
<li>Sum: <a href="https://issues.apache.org/jira/browse/SPARK-34716">SPARK-34716</a></li>
<li>DateAddYMInterval/(AddMonthsBase-&gt;AddMonths): <a href="https://issues.apache.org/jira/browse/SPARK-34721">SPARK-34721</a></li>
<li>TimeAdd: <a href="https://issues.apache.org/jira/browse/SPARK-34761">SPARK-34761</a> <a href="https://issues.apache.org/jira/browse/SPARK-35051">SPARK-35051</a></li>
<li>MultiplyYMInterval：<a href="https://issues.apache.org/jira/browse/SPARK-34824">SPARK-34824</a> </li>
<li>Average: <a href="https://issues.apache.org/jira/browse/SPARK-34837">SPARK-34837</a> </li>
<li>MultiplyDTInterval: <a href="https://issues.apache.org/jira/browse/SPARK-34850">SPARK-34850</a></li>
<li>DivideYMInterval/DivideDTInterval : <a href="https://issues.apache.org/jira/browse/SPARK-34868">SPARK-34868</a>  <a href="https://issues.apache.org/jira/browse/SPARK-34875">SPARK-34875</a></li>
<li>SubtractDates ： <a href="https://issues.apache.org/jira/browse/SPARK-34896">SPARK-34896</a> </li>
<li>extract/date_part ： <a href="https://issues.apache.org/jira/browse/SPARK-35091">SPARK-35091</a></li>
<li>SubtractTimestamps ： <a href="https://issues.apache.org/jira/browse/SPARK-34903">SPARK-34903</a></li>
<li>Sequence： <a href="https://issues.apache.org/jira/browse/SPARK-35088">SPARK-35088</a> </li>
<li>Hash：<a href="https://issues.apache.org/jira/browse/SPARK-35113">SPARK-35113</a> </li>
<li></li>
</ul>
</li>
<li><p>Hive相关：<a href="https://issues.apache.org/jira/browse/SPARK-35016">SPARK-35016</a> <a href="https://issues.apache.org/jira/browse/SPARK-35017">SPARK-35017</a> <a href="https://issues.apache.org/jira/browse/SPARK-35018">SPARK-35018</a> <a href="https://issues.apache.org/jira/browse/SPARK-35228">SPARK-35228</a> <a href="https://issues.apache.org/jira/browse/SPARK-35228">SPARK-35228</a></p>
</li>
</ul>
<p>从这个Task的背景已经相关SubTask的汇总来看，添加ANSI SQL INTERVAL 这条主线能够涵盖Spark SQL的每个知识点，对于想参与Spark社区开发的新人我来说，是一个非常好的学习实践的机会.</p>
<h2 id="核心SubTask"><a href="#核心SubTask" class="headerlink" title="核心SubTask"></a>核心SubTask</h2><h3 id="SPARK-27793"><a href="#SPARK-27793" class="headerlink" title="SPARK-27793"></a>SPARK-27793</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-27793">SPARK-27793</a> Add ANSI SQL day-time and year-month interval types</p>
<p>扩展Catalyst的数据类型，引入两个符合ANSI SQL标准的数据类型</p>
<ul>
<li><code>DayTimeIntervalType</code> </li>
<li><code>YearMonthIntervalType</code> </li>
</ul>
<p><a href="https://github.com/apache/spark/pull/31614">PR分析</a>：</p>
<ol>
<li><p>在<code>DataTypes.java</code>文件中增加获取数据类型<code>DayTimeIntervalType/YearMonthIntervalType</code> </p>
</li>
<li><p>定义<code>DayTimeIntervalType/YearMonthIntervalType</code> 数据类型</p>
<ul>
<li><p><code>DayTimeIntervalType</code>：</p>
<ol>
<li><p>内部使用<code>Long</code>来表示这个数据类型</p>
</li>
<li><p>计算规则：-/+ (24<em>60</em>60 * DAY + 60*60 * HOUR + 60 * MINUTE + SECOND) * 1000000</p>
<p>所以数值的单位是<code>microseconds</code></p>
</li>
</ol>
</li>
<li><p><code>YearMonthIntervalType</code> ：</p>
<ol>
<li>内部使用<code>Int</code>来表示这个数据类型</li>
<li>计算规则： -/+ (12 * YEAR + MONTH)，所以数值的单位是Month</li>
</ol>
</li>
</ul>
</li>
</ol>
<p>重点说明：</p>
<p><img src="/images/spark/image030.png" alt="image030"></p>
<p>根据SQL规范，INTERVAL类型的各个字段是不可以为负的，但是本身INTERVAL值可以为负。</p>
<h3 id="SPARK-34605"><a href="#SPARK-34605" class="headerlink" title="SPARK-34605"></a>SPARK-34605</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34605">SPARK-34605</a> Support java.time.Duration as an external type of the day-time interval type</p>
<p>该SubTask的目标是能够使并行数据集合中的<code>java.time.Duration</code>可以转成Catalyst的<code>DayTimeIntervalType</code>数据类型，更直观点说，该Task达到以下目的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; val ds &#x3D; Seq(java.time.Duration.ofDays(10)).toDS</span><br><span class="line">ds: org.apache.spark.sql.Dataset[java.time.Duration] &#x3D; [value: interval day to second]</span><br><span class="line"></span><br><span class="line">###注意：typeName已经由daytimeinterval变成了interval day to sceond</span><br><span class="line"></span><br><span class="line">scala&gt; ds.collect</span><br><span class="line">res0: Array[java.time.Duration] &#x3D; Array(PT240H)</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/apache/spark/pull/31729">PR分析</a></p>
<ol>
<li>添加了<code>DurationConverter</code>方法把<code>java.time.Duration</code>类型转成Catalyst内部DataType（DayTimeIntervalType）<ul>
<li><code>durationToMicros()</code>, 把<code>Duration</code>转成<code>microseconds</code>.用<code>Long</code>来表示.</li>
<li><code>microsToDuration()</code>,把<code>microseconds</code>转成<code>Duration</code>.返回<code>Duration</code>类型.</li>
</ul>
</li>
<li>提供了两个方法<code>createDeserializerForDuration</code>和<code>createSerializerForJavaDuration</code>方法，用于<code>Encoder</code> <code>DayTimeIntervalType</code>数据类型</li>
<li>扩展了<code>Literal</code>，从<code>java.time.Duration</code>类型构造<code>DayTimeIntervalType</code>类型</li>
</ol>
<p>附：这个SubTask比较核心，涉及了很多Catalyst的东西，包括数据类型转化，RowEncoder、数据读取、CodeGenerator等</p>
<h3 id="SPARK-34615"><a href="#SPARK-34615" class="headerlink" title="SPARK-34615"></a>SPARK-34615</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34615">SPARK-34615</a> Support java.time.Period as an external type of the year-month interval type</p>
<p><a href="https://github.com/apache/spark/pull/31765">PR分析</a></p>
<p>关于这个SubTask的分析，基本同上SPARK-34605</p>
<h3 id="SPARK-34667"><a href="#SPARK-34667" class="headerlink" title="SPARK-34667"></a>SPARK-34667</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34667">SPARK-34667</a> Support casting of year-month interval to string</p>
<p>该SubTask就是增加了把year-month interval 转成string，效果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; import org.apache.spark.sql.catalyst.expressions.&#123;Literal, Cast&#125;</span><br><span class="line">import org.apache.spark.sql.catalyst.expressions.&#123;Literal, Cast&#125;</span><br><span class="line"></span><br><span class="line">scala&gt; import org.apache.spark.sql.types._</span><br><span class="line">import org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line">scala&gt; Cast(Literal(java.time.Period.ofMonths(1)), StringType)</span><br><span class="line">res0: org.apache.spark.sql.catalyst.expressions.Cast &#x3D; cast(INTERVAL &#39;0-1&#39; YEAR TO MONTH as string)</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/apache/spark/pull/32056">PR分析</a></p>
<ol>
<li>扩展<code>Cast expression</code></li>
<li>在<code>IntervalUtils</code>中增加<code>toYearMonthIntervalString()</code>方法</li>
</ol>
<h3 id="SPARK-34668"><a href="#SPARK-34668" class="headerlink" title="SPARK-34668"></a>SPARK-34668</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34668">SPARK-34668</a> Support casting of day-time intervals to strings</p>
<p><a href="https://github.com/apache/spark/pull/32070">PR分析</a></p>
<p>同<a href="https://issues.apache.org/jira/browse/SPARK-34667">SPARK-34667</a></p>
<h3 id="SPARK-34677"><a href="#SPARK-34677" class="headerlink" title="SPARK-34677"></a>SPARK-34677</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34677">SPARK-34677</a> Support add and subtract of ANSI SQL intervals</p>
<p>该SubTask的目标是在<code>DayTimeIntervalType</code>和<code>YearMonthIntervalType</code>上支持 <code>+/-</code>运算</p>
<p><a href="https://github.com/apache/spark/pull/31789">PR分析</a></p>
<p>TODO</p>
<h3 id="SPARK-34716"><a href="#SPARK-34716" class="headerlink" title="SPARK-34716"></a>SPARK-34716</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34716">SPARK-34716</a> Support ANSI SQL intervals by the aggregate function sum</p>
<p>该SubTask的目标是在<code>sum</code>表达式中增加了<code>DayTimeIntervalType</code>和<code>YearMonthIntervalType</code>类型的支持。</p>
<p><a href="https://github.com/apache/spark/pull/32107">PR分析</a></p>
<h3 id="SPARK-34721"><a href="#SPARK-34721" class="headerlink" title="SPARK-34721"></a>SPARK-34721</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34721">SPARK-34721</a> Add an year-month interval to a date</p>
<p>该SubTask的目标是支持 <code>date</code> 加/减 <code>year-month interval</code>, 最终返回<code>date</code>类型</p>
<p><a href="https://github.com/apache/spark/pull/31812">PR分析</a></p>
<h3 id="SPARK-34739"><a href="#SPARK-34739" class="headerlink" title="SPARK-34739"></a>SPARK-34739</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34739">SPARK-34739</a> Add an year-month interval to a timestamp</p>
<p><a href="https://github.com/apache/spark/pull/31861">PR分析</a></p>
<h3 id="SPARK-34761"><a href="#SPARK-34761" class="headerlink" title="SPARK-34761"></a>SPARK-34761</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34761">SPARK-34761</a> Add a day-time interval to a timestamp</p>
<p>该SubTask实现了<code>timestamp</code> +/- <code>day-time interval</code>的功能</p>
<p><a href="https://github.com/apache/spark/pull/31855">PR分析</a></p>
<h3 id="SPARK-34824"><a href="#SPARK-34824" class="headerlink" title="SPARK-34824"></a>SPARK-34824</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34824">SPARK-34824</a> Multiply year-month interval by numeric</p>
<p>该SubTask新加了<code>MultiplyYMInterval</code>实现了 <code>numeric * year-month interval</code>的功能</p>
<p>同时扩展了<code>arithmetic rules</code>支持 <code>numeric * year-month interval</code> 和 <code>year-month interval * numeric</code></p>
<p><a href="https://github.com/apache/spark/pull/31929">PR分析</a></p>
<h3 id="SPARK-34837"><a href="#SPARK-34837" class="headerlink" title="SPARK-34837"></a>SPARK-34837</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34837">SPARK-34837</a> Support ANSI SQL intervals by the aggregate function avg</p>
<p>该SubTask扩展了<code>Average expression</code>来支持<code>DayTimeIntervalType</code>和<code>YearMonthIntervalType</code>数据类型</p>
<p><a href="https://github.com/apache/spark/pull/32358">PR分析</a></p>
<h3 id="SPARK-34850"><a href="#SPARK-34850" class="headerlink" title="SPARK-34850"></a>SPARK-34850</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34850">SPARK-34850</a> Multiply day-time interval by numeric</p>
<p>该SubTask新加了<code>MultiplyDTInterval</code>实现了 <code>numberic * day-time interval</code>的功能</p>
<p>同时扩展了<code>arithmetic rules</code>支持 <code>numeric * day-time interval</code> 和 <code>day-time interval * numeric</code></p>
<p><a href="https://github.com/apache/spark/pull/31951">PR分析</a></p>
<h3 id="SPARK-34868"><a href="#SPARK-34868" class="headerlink" title="SPARK-34868"></a>SPARK-34868</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34868">SPARK-34868</a> Divide year-month interval by numeric</p>
<p>该SubTask新加了<code>DivideYMInterval</code>实现了 <code>year-month interval / numberic</code></p>
<p>同时扩展了<code>arithmetic rules</code>支持<code>year-month interval / numberic</code></p>
<p><a href="https://github.com/apache/spark/pull/31961">PR分析</a></p>
<h3 id="SPARK-34875"><a href="#SPARK-34875" class="headerlink" title="SPARK-34875"></a>SPARK-34875</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34875">SPARK-34875</a> Divide day-time interval by numeric</p>
<p>该SubTask新加了<code>DivideDTInterval</code>，实现了<code>day-time interval / numeric</code>的功能</p>
<p>同时扩展了<code>arithmetic rules</code>支持<code>day-time interval / numeric</code></p>
<p><a href="https://github.com/apache/spark/pull/31972">PR分析</a></p>
<h3 id="SPARK-34879"><a href="#SPARK-34879" class="headerlink" title="SPARK-34879"></a>SPARK-34879</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34879">SPARK-34879</a> Hive inspect support DayTimeIntervalType and YearMonthIntervalType</p>
<p><a href="https://github.com/apache/spark/pull/31979">PR分析</a></p>
<h3 id="SPARK-34896"><a href="#SPARK-34896" class="headerlink" title="SPARK-34896"></a>SPARK-34896</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34896">SPARK-34896</a> Return day-time interval from dates subtraction</p>
<p>该SubTask修改了<code>SubtractDates expression</code>，同时增加了<code>spark.sql.legacy.interval.enabled</code>参数，在参数设置为false(默认)的情况下，<code>SubtractDates</code>返回的结果类型为<code>DayTimeIntervalType</code></p>
<p><a href="https://github.com/apache/spark/pull/31996">PR分析</a></p>
<h3 id="SPARK-35091"><a href="#SPARK-35091" class="headerlink" title="SPARK-35091"></a>SPARK-35091</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35091">SPARK-35091</a> Support ANSI intervals by date_part()</p>
<p>该SubTask扩展了<code>extract/date_part expression</code>，使得支持<code>ANSI Intervals</code></p>
<p><a href="https://github.com/apache/spark/pull/32351">PR分析</a></p>
<h3 id="SPARK-34903"><a href="#SPARK-34903" class="headerlink" title="SPARK-34903"></a>SPARK-34903</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34903">SPARK-34903</a> Return day-time interval from timestamps subtraction</p>
<p>该SubTask修改了<code>SubtractTimestamps expression</code>，使它返回的结果数据类型为<code>DayTimeIntervalType</code></p>
<p><a href="https://github.com/apache/spark/pull/32016">PR分析</a></p>
<h3 id="SPARK-35129"><a href="#SPARK-35129" class="headerlink" title="SPARK-35129"></a>SPARK-35129</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35129">SPARK-35129</a> Construct year-month interval column from integral fields</p>
<p>OPEN</p>
<h3 id="SPARK-35111"><a href="#SPARK-35111" class="headerlink" title="SPARK-35111"></a>SPARK-35111</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35111">SPARK-35111</a> Cast string to year-month interval</p>
<p><a href="https://github.com/apache/spark/pull/32444">PR分析</a></p>
<h3 id="SPARK-35037"><a href="#SPARK-35037" class="headerlink" title="SPARK-35037"></a>SPARK-35037</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35037">SPARK-35037</a> Recognize sign before the interval string in literals</p>
<p>扩展了SQL syntax rules，是它支持<code>year-month/day-time intervals</code> 前面带有负号.</p>
<p><a href="https://github.com/apache/spark/pull/32134">PR分析</a></p>
<p>在<code>visitUnitToUnitInterval</code>方法中增加负数的处理</p>
<h3 id="SPARK-35051"><a href="#SPARK-35051" class="headerlink" title="SPARK-35051"></a>SPARK-35051</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35051">SPARK-35051</a> Add a day-time interval to a date</p>
<p>该Subtask实现了 <code>date +/- day-time interval</code>的功能</p>
<p><a href="https://github.com/apache/spark/pull/32170">PR分析</a></p>
<ul>
<li>先把<code>date</code>转成<code>timestamp</code></li>
<li>然后<code>timestamp +/- day-time interval</code></li>
</ul>
<h3 id="SPARK-35076"><a href="#SPARK-35076" class="headerlink" title="SPARK-35076"></a>SPARK-35076</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35076">SPARK-35076</a> Parse interval literals as ANSI intervals</p>
<p>IN PROCESSING</p>
<h3 id="SPARK-35088"><a href="#SPARK-35088" class="headerlink" title="SPARK-35088"></a>SPARK-35088</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35088">SPARK-35088</a> Accept ANSI intervals by the Sequence expression</p>
<p><code>Sequence expression</code>支持<code>year-month/day-time interval</code> 类型</p>
<p><a href="https://github.com/apache/spark/pull/32492">PR分析</a></p>
<h3 id="SPARK-35090"><a href="#SPARK-35090" class="headerlink" title="SPARK-35090"></a>SPARK-35090</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35090">SPARK-35090</a> Extract a field from ANSI interval</p>
<p><a href="https://github.com/apache/spark/pull/32351">PR分析</a></p>
<p>IN PROGRESS</p>
<h3 id="SPARK-35099"><a href="#SPARK-35099" class="headerlink" title="SPARK-35099"></a>SPARK-35099</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35099">SPARK-35099</a> Convert ANSI interval literals to SQL string</p>
<p>在<code>Literal</code>中的<code>sql()</code>和<code>toString()</code>方法中增加了<code>YearMonthIntervalType</code>和<code>DayTimeIntervalType</code>的处理</p>
<p><a href="https://github.com/apache/spark/pull/32196">PR分析</a></p>
<h3 id="SPARK-35107"><a href="#SPARK-35107" class="headerlink" title="SPARK-35107"></a>SPARK-35107</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35107">SPARK-35017</a> Parse unit-to-unit interval literals to ANSI intervals</p>
<p>解析<code>unit-to-unit interval</code>成<code>YearMonthIntervalType</code>或者<code>DayTimeIntervalType</code></p>
<p><a href="https://github.com/apache/spark/pull/32209">PR分析</a></p>
<h3 id="SPARK-35110"><a href="#SPARK-35110" class="headerlink" title="SPARK-35110"></a>SPARK-35110</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35110">SPARK-35110</a> Handle ANSI intervals in WindowExecBase</p>
<p>该SubTask的目标是<code>WindowExecBase</code>支持<code>YearMonthIntervalType</code>和<code>DayTimeIntervalType</code>数据类型</p>
<p><a href="https://github.com/apache/spark/pull/32294">PR分析</a></p>
<h3 id="SPARK-35113"><a href="#SPARK-35113" class="headerlink" title="SPARK-35113"></a>SPARK-35113</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35113">SPARK-35113</a> Support ANSI intervals in the Hash expression</p>
<p>该SubTask在<code>Hash</code>表达式中增加了<code>YearMonthIntervalType</code>和<code>DayTimeIntervalType</code>的支持</p>
<p><a href="https://github.com/apache/spark/pull/32259">PR分析</a></p>
<h3 id="SPARK-35139"><a href="#SPARK-35139" class="headerlink" title="SPARK-35139"></a>SPARK-35139</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35139">SPARK-35139</a> Support ANSI intervals as Arrow Column vectors</p>
<p><a href="https://github.com/apache/spark/pull/32340">PR分析</a></p>
<h3 id="SPARK-35153"><a href="#SPARK-35153" class="headerlink" title="SPARK-35153"></a>SPARK-35153</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35153">SPARK-35153</a> Override sql() of ANSI interval operators</p>
<p>重写了<code>sql()</code>方法，使运算表达式更加可读，</p>
<p><a href="https://github.com/apache/spark/pull/32262">PR分析</a></p>
<ol>
<li><code>MultiplyYMInterval</code>中重写<code>sql()</code>方法</li>
<li><code>MultiplyDTInterval</code>中重写<code>sql()</code>方法</li>
<li><code>DivideYMInterval</code>中重写<code>sql()</code>方法</li>
<li><code>DivideDTInterval</code>中重写<code>sql()</code>方法</li>
<li><code>SubstractTimestamps</code>中重写<code>sql()</code>方法</li>
</ol>
<h3 id="SPARK-35243"><a href="#SPARK-35243" class="headerlink" title="SPARK-35243"></a>SPARK-35243</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35243">SPARK-35243</a> Support columnar execution on ANSI interval types</p>
<p><a href="https://github.com/apache/spark/pull/32452">PR分析</a></p>
<h3 id="SPARK-35285"><a href="#SPARK-35285" class="headerlink" title="SPARK-35285"></a>SPARK-35285</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35285">SPARK-35285</a> Parse ANSI interval types in SQL</p>
<p>该SubTask的目的是增加SQL Parser功能，使得SQL Parser可以解析：</p>
<ul>
<li><code>INTERVAL YEAR TO MONTH</code> to <code>YearMonthIntervalType</code></li>
<li><code>INTERVAL DAY TO SECOND</code> to <code>DayTimeIntervalType</code></li>
</ul>
<p><a href="https://github.com/apache/spark/pull/32409">PR分析</a></p>
<ol>
<li>在<code>SqlBase.g4</code>中增加 <code>INTERVAL YEAR TO MONTH / INTERVAL DAY TO SECOND</code>的解析规则</li>
<li>在<code>AstBuilder.scala</code>中增加<code>YearMonthIntervalDataType/DayTimeIntervalType</code>的访问</li>
</ol>
<p>Spark提供了一个SqlBase.g4文件，编译的时候，会使用Antlr根据这个文件生成对应的词法解析和语法分析类，同时还使用了访问者模式，用于构建Logical Plan（语法树）。</p>
<p>访问者模式简单说就是会去遍历生成的语法树（针对语法树中每个节点生成visit方法），以及返回相应的值。</p>
<p>Spark通过AstBuilder继承SqlBaseBaseVisitor类，重写了对应节点的visit方法，实现了自己的访问逻辑。</p>
<h2 id="其他SubTask"><a href="#其他SubTask" class="headerlink" title="其他SubTask"></a>其他SubTask</h2><h3 id="SPARK-34619"><a href="#SPARK-34619" class="headerlink" title="SPARK-34619"></a>SPARK-34619</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34619">SPARK-34619</a> Update the Spark SQL guide about day-time and year-month interval types</p>
<p>目前处于open状态</p>
<h3 id="SPARK-34663"><a href="#SPARK-34663" class="headerlink" title="SPARK-34663"></a>SPARK-34663</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34663">SPARK-34663</a> Test year-month and day-time intervals in UDF</p>
<p><a href="https://github.com/apache/spark/pull/31779">PR分析</a></p>
<p>其实这个也可以算是<a href="https://issues.apache.org/jira/browse/SPARK-34605">SPARK-34605</a>和<a href="https://issues.apache.org/jira/browse/SPARK-34615">SPARK-34615</a>的补充测试，在UDF中可以使用<code>Duration</code>和<code>Period</code>类型进行计算.</p>
<h3 id="SPARK-34666"><a href="#SPARK-34666" class="headerlink" title="SPARK-34666"></a>SPARK-34666</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34666">SPARK-34666</a> Test DayTimeIntervalType/YearMonthIntervalType as ordered and atomic types</p>
<p><a href="https://github.com/apache/spark/pull/31782">PR分析</a></p>
<p>首先在SPARK中<code>DataType</code>上的测试类型包括以下：</p>
<ol>
<li>ArithmeticExpressionSuite:<ul>
<li>“function least”</li>
<li>“function greatest”</li>
</ul>
</li>
<li>PredicateSuite<ul>
<li>“BinaryComparison consistency check”</li>
<li>“AND, OR, EqualTo, EqualNullSafe consistency check”</li>
</ul>
</li>
<li>ConditionalExpressionSuite<ul>
<li>“if”</li>
</ul>
</li>
<li>RandomDataGeneratorSuite<ul>
<li>“Basic types”</li>
</ul>
</li>
<li>CastSuite<ul>
<li>“null cast”</li>
<li>“up-cast”</li>
<li>“<a href="https://issues.apache.org/jira/browse/SPARK-27671">SPARK-27671</a>: cast from nested null type in struct”</li>
</ul>
</li>
<li>OrderingSuite<ul>
<li>“GenerateOrdering with DayTimeIntervalType”</li>
<li>“GenerateOrdering with YearMonthIntervalType”</li>
</ul>
</li>
<li>PredicateSuite<ul>
<li>“IN with different types”</li>
</ul>
</li>
<li>UnsafeRowSuite<ul>
<li>“calling get(ordinal, datatype) on null columns”</li>
</ul>
</li>
<li>SortSuite<ul>
<li>“sorting on YearMonthIntervalType …”</li>
<li>“sorting on DayTimeIntervalType …”</li>
</ul>
</li>
</ol>
<p>为了能够使<code>DayTimeIntervalType/YearMonthIntervalType</code>可以用于以上场景的测试</p>
<ol>
<li>在<code>DataTypeTestUtils.ordered</code>/<code>atomicTypes</code> 中增加<code>DayTimeIntervalType</code> 和<code>YearMonthIntervalType</code> </li>
<li>在<code>LiteralGenerator</code>/<code>RandomDataGenerator</code>中增加了case，来生成<code>DayTimeIntervalType</code> 和<code>YearMonthIntervalType</code> 类型的数据</li>
</ol>
<h3 id="SPARK-34695"><a href="#SPARK-34695" class="headerlink" title="SPARK-34695"></a>SPARK-34695</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34695">SPARK-34695</a> Overflow in round trip conversion from micros to duration</p>
<p>该SubTask的目标是解决micros和duration之间的转化产生溢出的问题</p>
<p><a href="https://github.com/apache/spark/pull/31799">PR分析</a></p>
<h3 id="SPARK-34715"><a href="#SPARK-34715" class="headerlink" title="SPARK-34715"></a>SPARK-34715</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34715">SPARK-34715</a> Add round trip tests for period &lt;-&gt; month and duration &lt;-&gt; micros</p>
<p>该SubTask的目标是增加了测试</p>
<ol>
<li>period &lt;-&gt; month &lt;-&gt; period</li>
<li>duration &lt;-&gt; micros  &lt;-&gt; duration </li>
</ol>
<p><a href="https://github.com/apache/spark/pull/32234">PR分析</a></p>
<h3 id="SPARK-34718"><a href="#SPARK-34718" class="headerlink" title="SPARK-34718"></a>SPARK-34718</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34718">SPARK-34718</a> Assign pretty names to YearMonthIntervalType and DayTimeIntervalType</p>
<p>该SubTask的目标是修改 <code>YearMonthIntervalType</code> 和 <code>DayTimeIntervalType</code> 的type name</p>
<p><a href="https://github.com/apache/spark/pull/31810/">PR分析</a></p>
<h3 id="SPARK-34793"><a href="#SPARK-34793" class="headerlink" title="SPARK-34793"></a>SPARK-34793</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34793">SPARK-34793</a> Prohibit saving of day-time and year-month intervals</p>
<p><a href="https://github.com/apache/spark/pull/31884">PR分析</a></p>
<h3 id="SPARK-34841"><a href="#SPARK-34841" class="headerlink" title="SPARK-34841"></a>SPARK-34841</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34841">SPARK-34841</a> Push ANSI interval binary expressions into into (if / case) branches</p>
<p><a href="https://github.com/apache/spark/pull/31978">PR分析</a></p>
<h3 id="SPARK-34878"><a href="#SPARK-34878" class="headerlink" title="SPARK-34878"></a>SPARK-34878</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34878">SPARK-34878</a> Test actual size of year-month and day-time intervals</p>
<p><a href="https://github.com/apache/spark/pull/32366">PR分析</a></p>
<h3 id="SPARK-34905"><a href="#SPARK-34905" class="headerlink" title="SPARK-34905"></a>SPARK-34905</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34905">SPARK-34905</a> Enable ANSI intervals in SQLQueryTestSuite</p>
<p><a href="https://github.com/apache/spark/pull/32099">PR分析</a></p>
<h3 id="SPARK-35018"><a href="#SPARK-35018" class="headerlink" title="SPARK-35018"></a>SPARK-35018</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35018">SPARK-35018</a> Test transferring year-month interval via Hive Thrift server</p>
<p><a href="https://github.com/apache/spark/pull/32240">PR分析</a></p>
<h3 id="SPARK-35085"><a href="#SPARK-35085" class="headerlink" title="SPARK-35085"></a>SPARK-35085</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35085">SPARK-35085</a> Get columns operation should handle ANSI interval column properly</p>
<p>支持JDBC的客户端能够正确识别ANSI interval类型的列</p>
<p><a href="https://github.com/apache/spark/pull/32345">PR分析</a></p>
<h3 id="SPARK-35177"><a href="#SPARK-35177" class="headerlink" title="SPARK-35177"></a>SPARK-35177</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35177">SPARK-35177</a> IntervalUtils.fromYearMonthString can’t handle Int.MinValue correctly</p>
<p><a href="https://github.com/apache/spark/pull/32281">PR分析</a></p>
<h3 id="SPARK-35095"><a href="#SPARK-35095" class="headerlink" title="SPARK-35095"></a>SPARK-35095</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35095">SPARK-35095</a> Use ANSI intervals in streaming join tests</p>
<h3 id="SPARK-35114"><a href="#SPARK-35114" class="headerlink" title="SPARK-35114"></a>SPARK-35114</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35114">SPARK-35114</a> Test ANSI interval literals</p>
<p>在LiteralExpressionSuite中增加了对YearMonthIntervalType和DayTimeIntervalType的测试</p>
<p><a href="https://github.com/apache/spark/pull/32213">PR分析</a></p>
<h3 id="SPARK-35115"><a href="#SPARK-35115" class="headerlink" title="SPARK-35115"></a>SPARK-35115</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35115">SPARK-35115</a> Test ANSI intervals in MutableProjectionSuite</p>
<p>在MutableProjectionSuite测试中增加了对YearMonthIntervalType和DayTimeIntervalType的测试</p>
<p><a href="https://github.com/apache/spark/pull/32225">PR分析</a></p>
<h3 id="SPARK-35116"><a href="#SPARK-35116" class="headerlink" title="SPARK-35116"></a>SPARK-35116</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35116">SPARK-35116</a> The generated data fits the precision of DayTimeIntervalType in spark</p>
<p>该SubTask修改了RandomDataGenerator中生成DayTimeIntervalType类型数据的精度，在Spark中DayTimeIntervalType的精度使微妙，在java.time.Duration中精度使纳秒</p>
<p><a href="https://github.com/apache/spark/pull/32212">PR分析</a></p>
<h3 id="SPARK-35068"><a href="#SPARK-35068" class="headerlink" title="SPARK-35068"></a>SPARK-35068</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35068">SPARK-35068</a> Add tests for ANSI intervals to HiveThriftBinaryServerSuite</p>
<p><a href="https://github.com/apache/spark/pull/32250">PR分析</a></p>
<h3 id="SPARK-35130"><a href="#SPARK-35130" class="headerlink" title="SPARK-35130"></a>SPARK-35130</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35130">SPARK-35130</a> Construct day-time interval column from integral fields</p>
<h3 id="SPARK-35169"><a href="#SPARK-35169" class="headerlink" title="SPARK-35169"></a>SPARK-35169</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35169">SPARK-35169</a> Wrong result of min ANSI interval division by -1</p>
<p><a href="https://github.com/apache/spark/pull/32314">PR分析</a></p>
<h3 id="SPARK-35187"><a href="#SPARK-35187" class="headerlink" title="SPARK-35187"></a>SPARK-35187</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35187">SPARK-35187</a> Failure on minimal interval literal</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark-sql&gt; SELECT INTERVAL &#39;-178956970-8&#39; YEAR TO MONTH;</span><br><span class="line">-178956970-8</span><br><span class="line"></span><br><span class="line">spark-sql&gt; SELECT INTERVAL -&#39;178956970-8&#39; YEAR TO MONTH;</span><br><span class="line">Error in query:</span><br><span class="line">Error parsing interval year-month string: integer overflow(line 1, pos 16)</span><br></pre></td></tr></table></figure>
<p>处理符号位在Interval string外边的时候，产生异常的场景</p>
<p><a href="https://github.com/apache/spark/pull/32296">PR分析</a></p>
<ol>
<li>AstBuilder.scala中处理了读取数据的逻辑</li>
<li>在interval.sql中增加了测试case</li>
</ol>
<h3 id="SPARK-35228"><a href="#SPARK-35228" class="headerlink" title="SPARK-35228"></a>SPARK-35228</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35228">SPARK-35228</a> Add expression ToHiveString for keep consistent between hive/spark format in df.show and transform</p>
<p>IN PROGRESS</p>
<p><a href="https://github.com/apache/spark/pull/32365">PR分析</a></p>
<h3 id="SPARK-35112"><a href="#SPARK-35112" class="headerlink" title="SPARK-35112"></a>SPARK-35112</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35112">SPARK-35112</a> Cast string to day-time interval</p>
<p>该SubTask是<a href="https://issues.apache.org/jira/browse/SPARK-35111">SPARK-35111</a> 的补充</p>
<p><a href="https://github.com/apache/spark/pull/32444">PR分析</a></p>
<h3 id="SPARK-34984"><a href="#SPARK-34984" class="headerlink" title="SPARK-34984"></a>SPARK-34984</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-34984">SPARK-34984</a> ANSI intervals formatting in hive results</p>
<p><a href="https://github.com/apache/spark/pull/32087">PR分析</a></p>
<h3 id="SPARK-35016"><a href="#SPARK-35016" class="headerlink" title="SPARK-35016"></a>SPARK-35016</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35016">SPARK-35016</a> Format ANSI intervals in Hive style</p>
<p><a href="https://github.com/apache/spark/pull/32120">PR分析</a></p>
<h3 id="SPARK-35017"><a href="#SPARK-35017" class="headerlink" title="SPARK-35017"></a>SPARK-35017</h3><p><a href="https://issues.apache.org/jira/browse/SPARK-35017">SPARK-35017</a> Transfer ANSI intervals via Hive Thrift server</p>
<p><a href="https://github.com/apache/spark/pull/32121">PR分析</a></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>TODO</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>SQL standard for INTERVAL</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;interval literal&gt; ::&#x3D;</span><br><span class="line">  INTERVAL [ &lt;sign&gt; ] &lt;interval string&gt; &lt;interval qualifier&gt;</span><br><span class="line"></span><br><span class="line">&lt;interval qualifier&gt; ::&#x3D;</span><br><span class="line">    &lt;start field&gt; TO &lt;end field&gt;</span><br><span class="line">  | &lt;single datetime field&gt;</span><br><span class="line">&lt;start field&gt; ::&#x3D;</span><br><span class="line">&lt;non-second primary datetime field&gt;</span><br><span class="line">      [ &lt;left paren&gt; &lt;interval leading field precision&gt; &lt;right paren&gt; ]</span><br><span class="line">&lt;end field&gt; ::&#x3D;</span><br><span class="line">&lt;non-second primary datetime field&gt;</span><br><span class="line">  | SECOND [ &lt;left paren&gt; &lt;interval fractional seconds precision&gt; &lt;right paren&gt; ]</span><br><span class="line">&lt;single datetime field&gt; ::&#x3D; &lt;non-second primary datetime field&gt;</span><br><span class="line">        [ &lt;left paren&gt; &lt;interval leading field precision&gt; &lt;right paren&gt; ]</span><br><span class="line">  | SECOND [ &lt;left paren&gt; &lt;interval leading field precision&gt;</span><br><span class="line">      [ &lt;comma&gt; &lt;interval fractional seconds precision&gt; ] &lt;right paren&gt; ]</span><br><span class="line">&lt;primary datetime field&gt; ::&#x3D; &lt;non-second primary datetime field&gt;</span><br><span class="line">| SECOND</span><br><span class="line">&lt;non-second primary datetime field&gt; ::&#x3D; YEAR</span><br><span class="line">  | MONTH</span><br><span class="line">  | DAY</span><br><span class="line">  | HOUR</span><br><span class="line">  | MINUTE</span><br><span class="line"></span><br><span class="line">&lt;interval string&gt; ::&#x3D;</span><br><span class="line">  &lt;quote&gt; &lt;unquoted interval string&gt; &lt;quote&gt;</span><br><span class="line">&lt;unquoted interval string&gt; ::&#x3D;</span><br><span class="line">[ &lt;sign&gt; ] &#123; &lt;year-month literal&gt; | &lt;day-time literal&gt; &#125;</span><br></pre></td></tr></table></figure>


<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
]]></content>
      <categories>
        <category>Spark SQL系列</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
</search>
