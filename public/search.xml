<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>SparkEnv机制</title>
    <url>/2021/02/23/SparkEnv%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>一段来自代码的注释：</p>
<p>“ <em>Holds all the runtime environment objects for a running Spark instance (either master or worker),including the serializer, RpcEnv, block manager, map output tracker, etc. Currently Spark code finds the SparkEnv through a global variable, so all the threads can access the same SparkEnv. It can be accessed by SparkEnv.get (e.g. after creating a SparkContext).</em> ”</p>
<a id="more"></a>

<p>SparkEnv是SparkContext对象中最重要的一个属性，保存着Spark运行实例的所有运行环境信息，包括序列化、RpcEnv，block 管理等。关于SparkContext这个大类我们后续分析。</p>
<h2 id="SparkEnv"><a href="#SparkEnv" class="headerlink" title="SparkEnv"></a>SparkEnv</h2><p>SparkEnv类的描述</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@DeveloperApi</span><br><span class="line">class SparkEnv (</span><br><span class="line">    val executorId: String,</span><br><span class="line">    private[spark] val rpcEnv: RpcEnv,</span><br><span class="line">    val serializer: Serializer,</span><br><span class="line">    val closureSerializer: Serializer,</span><br><span class="line">    val serializerManager: SerializerManager,</span><br><span class="line">    val mapOutputTracker: MapOutputTracker,</span><br><span class="line">    val shuffleManager: ShuffleManager,</span><br><span class="line">    val broadcastManager: BroadcastManager,</span><br><span class="line">    val blockManager: BlockManager,</span><br><span class="line">    val securityManager: SecurityManager,</span><br><span class="line">    val metricsSystem: MetricsSystem,</span><br><span class="line">    val memoryManager: MemoryManager,</span><br><span class="line">    val outputCommitCoordinator: OutputCommitCoordinator,</span><br><span class="line">    val conf: SparkConf) extends Logging &#123;</span><br><span class="line">    ...    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从SparkEnv的类的描述中可以看到SparkEnv一共包含了12个重要的组件</p>
<ul>
<li>RpcEnv 各组件间的通信环境信息</li>
<li>SerializerManager 对象的序列化管理器</li>
<li>MapOutputTracker 用于跟踪Map阶段任务的输出状态，此状态便于Reduce阶段任务获取地址及中间结果</li>
<li>ShuffleManager 负责Shuffle操作，包括Shuffle read和Shuffle write</li>
<li>BroadcastManager 广播变量管理器</li>
<li>BlockManager 块管理器，在Spark中Block可以理解为RDD的一个Partition</li>
<li>SecurityManager 主要对账户、权限及身份认证进行设置和管理</li>
<li>MetricsSystem 监控指标管理</li>
<li>MemoryManager 内存管理器，可以参考之前的Spark内存管理机制</li>
<li>OutputCommitCoordinator 确定任务是否可以把输出提到HDFS上</li>
</ul>
<p>SparkEnv从逻辑上可以分成DriverEnv还是ExecutorEnv</p>
<p>从堆栈中可以看到不管是DriverEnv还是ExecutorEnv最终的创建都会执行SparkEnv类中的create方法，正如代码中的注释所描述的：</p>
<p>“*Helper method to create a SparkEnv for a driver or an executor.*”</p>
<p><img src="/images/spark/image004.png"></p>
<p>整个create的过程就是SparkEnv所holds的运行环境信息对应manager的初始过程。</p>
<h2 id="SparkEnv的创建"><a href="#SparkEnv的创建" class="headerlink" title="SparkEnv的创建"></a>SparkEnv的创建</h2><p>前面章节也说了SparkEnv的创建就是每个组件的创建过程。挑重点学习一下。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val envInstance &#x3D; new SparkEnv(</span><br><span class="line">  executorId,</span><br><span class="line">  rpcEnv,</span><br><span class="line">  serializer,</span><br><span class="line">  closureSerializer,</span><br><span class="line">  serializerManager,</span><br><span class="line">  mapOutputTracker,</span><br><span class="line">  shuffleManager,</span><br><span class="line">  broadcastManager,</span><br><span class="line">  blockManager,</span><br><span class="line">  securityManager,</span><br><span class="line">  metricsSystem,</span><br><span class="line">  memoryManager,</span><br><span class="line">  outputCommitCoordinator,</span><br><span class="line">  conf)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Add a reference to tmp dir created by driver, we will delete this tmp dir when stop() </span><br><span class="line">   is</span><br><span class="line">&#x2F;&#x2F; called, and we only need to do it for driver. Because driver may run as a service, and </span><br><span class="line">   if we</span><br><span class="line">&#x2F;&#x2F; don&#39;t delete this tmp dir when sc is stopped, then will create too many tmp dirs.</span><br><span class="line">if (isDriver) &#123;</span><br><span class="line">  val sparkFilesDir &#x3D; Utils.createTempDir(Utils.getLocalDir(conf), </span><br><span class="line">                                          &quot;userFiles&quot;).getAbsolutePath</span><br><span class="line">  envInstance.driverTmpDir &#x3D; Some(sparkFilesDir)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">envInstance</span><br></pre></td></tr></table></figure>
<h3 id="广播管理器BroadcastManager"><a href="#广播管理器BroadcastManager" class="headerlink" title="广播管理器BroadcastManager"></a>广播管理器BroadcastManager</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val broadcastManager &#x3D; new BroadcastManager(isDriver, conf, securityManager)</span><br></pre></td></tr></table></figure>
<h3 id="Map任务输出跟踪器MapOutputTracker"><a href="#Map任务输出跟踪器MapOutputTracker" class="headerlink" title="Map任务输出跟踪器MapOutputTracker"></a>Map任务输出跟踪器MapOutputTracker</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val mapOutputTracker &#x3D; if (isDriver) &#123;</span><br><span class="line">  new MapOutputTrackerMaster(conf, broadcastManager, isLocal)</span><br><span class="line">&#125; else &#123;</span><br><span class="line">  new MapOutputTrackerWorker(conf)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Have to assign trackerEndpoint after initialization as MapOutputTrackerEndpoint</span><br><span class="line">&#x2F;&#x2F; requires the MapOutputTracker itself</span><br><span class="line">mapOutputTracker.trackerEndpoint &#x3D; registerOrLookupEndpoint(MapOutputTracker.ENDPOINT_NAME,</span><br><span class="line">  new MapOutputTrackerMasterEndpoint(</span><br><span class="line">	rpcEnv, mapOutputTracker.asInstanceOf[MapOutputTrackerMaster], conf))</span><br></pre></td></tr></table></figure>
<h3 id="Shuffle管理器ShuffleManager"><a href="#Shuffle管理器ShuffleManager" class="headerlink" title="Shuffle管理器ShuffleManager"></a>Shuffle管理器ShuffleManager</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; Let the user specify short names for shuffle managers</span><br><span class="line">val shortShuffleMgrNames &#x3D; Map(</span><br><span class="line">  &quot;sort&quot; -&gt; classOf[org.apache.spark.shuffle.sort.SortShuffleManager].getName,</span><br><span class="line">  &quot;tungsten-sort&quot; -&gt; classOf[org.apache.spark.shuffle.sort.SortShuffleManager].getName)</span><br><span class="line">val shuffleMgrName &#x3D; conf.get(config.SHUFFLE_MANAGER)</span><br><span class="line">val shuffleMgrClass &#x3D;</span><br><span class="line">  shortShuffleMgrNames.getOrElse(shuffleMgrName.toLowerCase(Locale.ROOT), shuffleMgrName)</span><br><span class="line">val shuffleManager &#x3D; instantiateClass[ShuffleManager](shuffleMgrClass)</span><br></pre></td></tr></table></figure>
<h3 id="内存管理器MemoryManager"><a href="#内存管理器MemoryManager" class="headerlink" title="内存管理器MemoryManager"></a>内存管理器MemoryManager</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val memoryManager: MemoryManager &#x3D; UnifiedMemoryManager(conf, numUsableCores)</span><br></pre></td></tr></table></figure>
<h3 id="BlockManagerMaster"><a href="#BlockManagerMaster" class="headerlink" title="BlockManagerMaster"></a>BlockManagerMaster</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; Mapping from block manager id to the block manager&#39;s information.</span><br><span class="line">val blockManagerInfo &#x3D; new concurrent.TrieMap[BlockManagerId, BlockManagerInfo]()</span><br><span class="line">val blockManagerMaster &#x3D; new BlockManagerMaster(</span><br><span class="line">  registerOrLookupEndpoint(</span><br><span class="line">	BlockManagerMaster.DRIVER_ENDPOINT_NAME,</span><br><span class="line">	new BlockManagerMasterEndpoint(</span><br><span class="line">	  rpcEnv,</span><br><span class="line">	  isLocal,</span><br><span class="line">	  conf,</span><br><span class="line">	  listenerBus,</span><br><span class="line">	  if (conf.get(config.SHUFFLE_SERVICE_FETCH_RDD_ENABLED)) &#123;</span><br><span class="line">		externalShuffleClient</span><br><span class="line">	  &#125; else &#123;</span><br><span class="line">		None</span><br><span class="line">	  &#125;, blockManagerInfo,</span><br><span class="line">	  mapOutputTracker.asInstanceOf[MapOutputTrackerMaster])),</span><br><span class="line">  registerOrLookupEndpoint(</span><br><span class="line">	BlockManagerMaster.DRIVER_HEARTBEAT_ENDPOINT_NAME,</span><br><span class="line">	new BlockManagerMasterHeartbeatEndpoint(rpcEnv, isLocal, blockManagerInfo)),</span><br><span class="line">  conf,</span><br><span class="line">  isDriver)</span><br></pre></td></tr></table></figure>
<h3 id="BlockManager"><a href="#BlockManager" class="headerlink" title="BlockManager"></a>BlockManager</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; NB: blockManager is not valid until initialize() is called later.</span><br><span class="line">val blockManager &#x3D; new BlockManager(</span><br><span class="line">  executorId,</span><br><span class="line">  rpcEnv,</span><br><span class="line">  blockManagerMaster,</span><br><span class="line">  serializerManager,</span><br><span class="line">  conf,</span><br><span class="line">  memoryManager,</span><br><span class="line">  mapOutputTracker,</span><br><span class="line">  shuffleManager,</span><br><span class="line">  blockTransferService,</span><br><span class="line">  securityManager,</span><br><span class="line">  externalShuffleClient)</span><br></pre></td></tr></table></figure>


<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
<p><a href="https://cnblogs.com/xia520pi/p/8609625.html">SparkEnv详解</a></p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark-Env</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark内存管理机制</title>
    <url>/2021/02/22/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Spark 作为一个基于内存的分布式计算引擎，其内存管理模块在整个系统中扮演着非常重要的角色。理解 Spark 内存管理的基本原理，有助于更好地开发 Spark 应用程序和进行性能调优。</p>
<a id="more"></a>

<p>在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能。</p>
<p>Spark内存管理代码主要代码分布在org.apache.spark.memory包下。</p>
<h2 id="内存分类"><a href="#内存分类" class="headerlink" title="内存分类"></a>内存分类</h2><p>在Spark中关于内存管理，入口类：<strong>MemoryManager</strong></p>
<p>从MemoryManager中可以总结出Spark的内存一共有两种划分方式：</p>
<ol>
<li><p>从内存的作用来划分（MemoryPool）；</p>
<p>MemoryPool的子类有ExecutionMemoryPool和StorageMemoryPool，分别用于execution </p>
<p>memory 和 storage memory的内存池</p>
</li>
<li><p>从内存的位置来划分（MemoryAllocator）；</p>
<p>ON_HEAP memory and OFF_HEAP memory，所以MemoryAllocator的实现类有HeapMemoryAllocator和UnsafeMemoryAllocator</p>
</li>
</ol>
<h2 id="MemoryManager的实现"><a href="#MemoryManager的实现" class="headerlink" title="MemoryManager的实现"></a>MemoryManager的实现</h2><p>UnifiedMemoryManager是目前Spark中唯一的MemoryManager的实现，之前的静态内存管理方案在Spark 1.6中被废弃了。</p>
<p>MemoryManager定义了主要的内存管理接口</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def acquireStorageMemory(blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean</span><br><span class="line"></span><br><span class="line">def acquireUnrollMemory(blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean</span><br><span class="line"></span><br><span class="line">def acquireExecutionMemory(numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long</span><br><span class="line"></span><br><span class="line">def releaseStorageMemory(numBytes: Long, memoryMode: MemoryMode): Unit</span><br><span class="line"></span><br><span class="line">def releaseExecutionMemory(numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Unit</span><br><span class="line"></span><br><span class="line">def releaseUnrollMemory(numBytes: Long, memoryMode: MemoryMode): Unit</span><br></pre></td></tr></table></figure>
<p>UnifiedMemoryManager机制</p>
<ul>
<li>设定基本的存储内存和执行内存区域（spark.storage.storageFraction 参数），该设定确定了双方各自拥有的空间的范围</li>
<li>双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block）</li>
<li>执行内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间</li>
<li>存储内存的空间被对方占用后，无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂</li>
</ul>
<p><img src="/images/spark/image001.png"></p>
<h2 id="Spark中的内存使用"><a href="#Spark中的内存使用" class="headerlink" title="Spark中的内存使用"></a>Spark中的内存使用</h2><p>在前面Spark内存的分类中，已经了解到Spark的内存按照作用来分，可以分为Execution memory and Storage memory，依次分析如下：</p>
<h3 id="Execution-memory"><a href="#Execution-memory" class="headerlink" title="Execution memory"></a>Execution memory</h3><p>来自代码的注释：</p>
<p><em>execution memory refers to that used for computation in shuffles, joins, sorts and aggregations</em></p>
<p>Execution memory简单点说就是用于Task自身执行过程中使用到的内存和用于shuffle数据的内存。</p>
<h4 id="Task执行内存"><a href="#Task执行内存" class="headerlink" title="Task执行内存"></a>Task执行内存</h4><p>来自代码的注释：</p>
<p>“<em>Tries to ensure that each task gets a reasonable share of memory, instead of some task ramping up to a large amount first and then causing others to spill to disk repeatedly.</em><br><em>If there are N tasks, it ensures that each task can acquire at least 1 / 2N of the memory before it has to spill, and at most 1 / N. Because N varies dynamically, we keep track of the set of active tasks and redo the calculations of 1 / 2N and 1 / N in waiting tasks whenever this set changes. This is all done by synchronizing access to mutable state and using wait() and notifyAll() to signal changes to callers. Prior to Spark 1.6, this arbitration of memory across tasks was performed by the ShuffleMemoryManager.</em>“</p>
<p>Executor 内运行的任务同样共享执行内存，Spark 用一个 HashMap 结构保存了任务到内存耗费的映射。每个任务可占用的执行内存大小的范围为 1/2N ~ 1/N，其中 N 为当前 Executor 内正在运行的任务的个数。每个任务在启动之时，要向 MemoryManager 请求申请最少为 1/2N 的执行内存，如果不能被满足要求则该任务被阻塞，直到有其他任务释放了足够的执行内存，该任务才可以被唤醒。这块儿的内存管理由TaskMemoryManager管理</p>
<h4 id="Shuffle内存"><a href="#Shuffle内存" class="headerlink" title="Shuffle内存"></a>Shuffle内存</h4><p>关于Shuffle内存这块儿的逻辑，可参考后续Shuffle模块儿的分析</p>
<h3 id="Storage-memory"><a href="#Storage-memory" class="headerlink" title="Storage memory"></a>Storage memory</h3><p>来自代码的注释：</p>
<p><em>storage memory refers to that used for caching and propagating internal data across the cluster</em></p>
<p>Storage memory的设计，可以对缓存 RDD 时使用的内存做统一的规划和管理同时作用于缓存 broadcast 数据。</p>
<p>RDD的缓存管理由Spark的Storage 模块儿负责实现，其中缓存的级别不仅仅包含内存，还包括Disk、远端存储等，同时还有缓存策略以及淘汰策略等逻辑。可以参考后续博客了解。</p>
<h2 id="Spark中内存Mode"><a href="#Spark中内存Mode" class="headerlink" title="Spark中内存Mode"></a>Spark中内存Mode</h2><p>在前面Spark内存的分类中，已经了解到Spark的内存位置来分可以分为：ON_HEAP memory and OFF_HEAP memory</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public enum MemoryMode &#123;</span><br><span class="line">  ON_HEAP,</span><br><span class="line">  OFF_HEAP</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其实就是两种MemoryAllocator</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MemoryAllocator UNSAFE &#x3D; new UnsafeMemoryAllocator();</span><br><span class="line">MemoryAllocator HEAP &#x3D; new HeapMemoryAllocator();</span><br></pre></td></tr></table></figure>
<p>MemoryAllocator的接口如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MemoryBlock allocate(long size) throws OutOfMemoryError;</span><br><span class="line">void free(MemoryBlock memory);</span><br></pre></td></tr></table></figure>
<h3 id="堆内内存"><a href="#堆内内存" class="headerlink" title="堆内内存"></a>堆内内存</h3><p>堆内内存其实就是使用HeapMemoryAllocator进行分配的内存</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * A simple &#123;@link MemoryAllocator&#125; that can allocate up to 16GB using a JVM long</span><br><span class="line"> * primitive array.</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class HeapMemoryAllocator implements MemoryAllocator &#123;</span><br><span class="line"></span><br><span class="line">  @GuardedBy(&quot;this&quot;)</span><br><span class="line">  private final Map&lt;Long, LinkedList&lt;WeakReference&lt;long[]&gt;&gt;&gt; bufferPoolsBySize &#x3D; new </span><br><span class="line">                                                                        HashMap&lt;&gt;();</span><br><span class="line">  ......  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/images/spark/image002.png"></p>
<p>Spark 对堆内内存的管理是一种逻辑上的”规划式”的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前 记录 这些内存。Spark是不能精准控制堆内内存的申请和释放，所以不能完全避免内存溢出，只是一定程度的减少了异常的出现。</p>
<h3 id="堆外内存"><a href="#堆外内存" class="headerlink" title="堆外内存"></a>堆外内存</h3><p>堆内内存其实就是使用UnsafeMemoryAllocator进行分配的内存</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class UnsafeMemoryAllocator implements MemoryAllocator &#123;</span><br><span class="line">  @Override</span><br><span class="line">  public MemoryBlock allocate(long size) throws OutOfMemoryError &#123;</span><br><span class="line">    long address &#x3D; Platform.allocateMemory(size);</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">  @Override</span><br><span class="line">  public void free(MemoryBlock memory) &#123;</span><br><span class="line">    ...</span><br><span class="line">    Platform.freeMemory(memory.offset);</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其核心是调用Platform中JDK Unsafe API，关于Platform的实现可参考后续博客</p>
<p><img src="/images/spark/image003.png"></p>
<p>Spark通过JDK Unsafe API实现了堆外内存的管理, 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。<a href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html">具体参考tungsten计划</a></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文从MemoryManager（根入口）结合 UnifiedMemoryManager（MemoryManager的唯一实现）的逻辑，从内存位置方面，分析了（ON_HEAP和OFF_HEAP），从内存使用方面，分析了（Execution memory and Storage memory）。</p>
<p>最核心的当然是MemoryManager，跟MemoryManager的交互却是TaskMemoryManager。</p>
<p>来自代码的注释：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">                                                       +---------------------------+</span><br><span class="line">+-------------+                                        |       MemoryManager       |</span><br><span class="line">| MemConsumer |----+                                   |                           |</span><br><span class="line">+-------------+    |    +-------------------+          |  +---------------------+  |</span><br><span class="line">                   +---&gt;| TaskMemoryManager |----+     |  |OnHeapStorageMemPool |  |</span><br><span class="line">+-------------+    |    +-------------------+    |     |  +---------------------+  |</span><br><span class="line">| MemConsumer |----+                             |     |                           |</span><br><span class="line">+-------------+         +-------------------+    |     |  +---------------------+  |</span><br><span class="line">                        | TaskMemoryManager |----+     |  |OffHeapStorageMemPool|  |</span><br><span class="line">                        +-------------------+    |     |  +---------------------+  |</span><br><span class="line">                                                 +----&gt;|                           |</span><br><span class="line">                                 *               |     |  +---------------------+  |</span><br><span class="line">                                 *               |     |  |OnHeapExecMemPool    |  |</span><br><span class="line">+-------------+                  *               |     |  +---------------------+  |</span><br><span class="line">| MemConsumer |----+                             |     |                           |</span><br><span class="line">+-------------+    |    +-------------------+    |     |  +---------------------+  |</span><br><span class="line">                   +---&gt;| TaskMemoryManager |----+     |  |OffHeapExecMemPool   |  |</span><br><span class="line">                        +-------------------+          |  +---------------------+  |</span><br><span class="line">                                                       |                           |</span><br><span class="line">                                                       +---------------------------+</span><br></pre></td></tr></table></figure>


<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/apache/spark">Spark代码</a></p>
<p><a href="https://developer.ibm.com/zh/articles/ba-cn-apache-spark-memory-management/">Apache Spark 内存管理详解</a></p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark-Memory</tag>
      </tags>
  </entry>
</search>
